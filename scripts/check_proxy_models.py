"""
æ¨¡å‹æ£€æŸ¥å·¥å…·

å¿«é€Ÿæ£€æŸ¥è®­ç»ƒå¥½çš„è¾…åŠ©æ¨¡å‹çš„çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡

ä½¿ç”¨æ–¹æ³•:
    python scripts/check_proxy_models.py [--model-dir models/proxy_models]

ä½œè€…: HEACé¡¹ç›®ç»„
"""

import sys
import argparse
from pathlib import Path
import joblib
import pandas as pd

def main():
    parser = argparse.ArgumentParser(description='æ£€æŸ¥è¾…åŠ©æ¨¡å‹çŠ¶æ€')
    parser.add_argument('--model-dir', type=str, default='models/proxy_models',
                       help='æ¨¡å‹ç›®å½•è·¯å¾„')
    args = parser.parse_args()
    
    model_dir = Path(args.model_dir)
    
    print("=" * 80)
    print(f"ğŸ“¦ æ£€æŸ¥æ¨¡å‹ç›®å½•: {model_dir}")
    print("=" * 80)
    
    if not model_dir.exists():
        print(f"\nâŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨")
        print(f"   è¯·å…ˆè®­ç»ƒæ¨¡å‹: python scripts/train_proxy_models.py")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    model_files = {
        'formation_energy': 'formation_energy_model.pkl',
        'lattice': 'lattice_model.pkl',
        'magnetic_moment': 'magnetic_moment_model.pkl',
        'bulk_modulus': 'bulk_modulus_model.pkl',
        'shear_modulus': 'shear_modulus_model.pkl',
        'brittleness': 'brittleness_model.pkl'
    }
    
    print("\nğŸ“‚ æ¨¡å‹æ–‡ä»¶:")
    found_models = []
    for model_name, filename in model_files.items():
        file_path = model_dir / filename
        if file_path.exists():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"   âœ… {model_name:20s} ({size_mb:.2f} MB)")
            found_models.append(model_name)
        else:
            print(f"   âŒ {model_name:20s} (ä¸å­˜åœ¨)")
    
    # æ£€æŸ¥ç‰¹å¾æ–‡ä»¶
    print("\nğŸ“„ è¾…åŠ©æ–‡ä»¶:")
    feature_file = model_dir / "feature_names.pkl"
    if feature_file.exists():
        feature_names = joblib.load(feature_file)
        print(f"   âœ… feature_names.pkl ({len(feature_names)} ä¸ªç‰¹å¾)")
    else:
        print(f"   âŒ feature_names.pkl")
    
    # æ£€æŸ¥è¯„ä¼°æŒ‡æ ‡
    metrics_file = model_dir / "metrics.pkl"
    if metrics_file.exists():
        metrics = joblib.load(metrics_file)
        print(f"   âœ… metrics.pkl ({len(metrics)} ä¸ªæ¨¡å‹çš„æŒ‡æ ‡)")
        
        # æ˜¾ç¤ºè¯¦ç»†æŒ‡æ ‡
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
        print("-" * 80)
        
        summary_data = []
        for model_name in found_models:
            if model_name in metrics:
                m = metrics[model_name]
                if isinstance(m, dict) and 'mae' in m:
                    summary_data.append({
                        'æ¨¡å‹': model_name,
                        'ç›®æ ‡': m.get('target_name', 'N/A'),
                        'MAE': f"{m['mae']:.4f}",
                        'RMSE': f"{m['rmse']:.4f}",
                        'RÂ²': f"{m['r2']:.4f}"
                    })
        
        if summary_data:
            df_summary = pd.DataFrame(summary_data)
            print(df_summary.to_string(index=False))
        else:
            print("   æ— æ³•è¯»å–æŒ‡æ ‡æ•°æ®")
    else:
        print(f"   âŒ metrics.pkl")
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print(f"âœ… æ‰¾åˆ° {len(found_models)}/{len(model_files)} ä¸ªæ¨¡å‹")
    
    if len(found_models) > 0:
        print(f"\nğŸ’¡ ä½¿ç”¨è¿™äº›æ¨¡å‹:")
        print(f"   from core.feature_injector import FeatureInjector")
        print(f"   injector = FeatureInjector(model_dir='{args.model_dir}')")
        print(f"   df_enhanced = injector.inject_features(df, comp_col='binder_composition')")
    else:
        print(f"\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        print(f"   è¯·å…ˆè®­ç»ƒæ¨¡å‹: python scripts/train_proxy_models.py")
    
    print("=" * 80)


if __name__ == "__main__":
    main()
