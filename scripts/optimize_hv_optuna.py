"""
é˜¶æ®µä¸‰ï¼šåŸºäºOptunaçš„è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–

æ ¸å¿ƒåˆ›æ–°ï¼šä¼˜åŒ–ç›®æ ‡åŒ…å«"æ³›åŒ–çº¦æŸ"
- ä¸å•çº¯è¿½æ±‚æœ€å°RMSE
- åŒæ—¶æƒ©ç½šè¿‡æ‹Ÿåˆ(train-val gap)

ä½¿ç”¨:
    python scripts/optimize_hv_optuna.py --n_trials 100 --algorithm xgboost
    python scripts/optimize_hv_optuna.py --n_trials 100 --algorithm catboost
    
ä½œè€…: HEACé«˜è´¨é‡ä¼˜åŒ–
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import optuna
    from optuna.visualization import plot_optimization_history, plot_param_importances
    OPTUNA_AVAILABLE = True
except ImportError:
    print("âš ï¸ Optunaæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install optuna")
    OPTUNA_AVAILABLE = False

from xgboost import XGBRegressor
try:
    from catboost import CatBoostRegressor
    CATBOOST_AVAILABLE = True
except ImportError:
    print("âš ï¸ CatBoostæœªå®‰è£…ï¼Œä»…ä½¿ç”¨XGBoost")
    CATBOOST_AVAILABLE = False

try:
    from lightgbm import LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except ImportError:
    print("âš ï¸ LightGBMæœªå®‰è£…")
    LIGHTGBM_AVAILABLE = False

from sklearn.model_selection import cross_validate, KFold
from sklearn.metrics import mean_squared_error, r2_score


def create_objective(X, y, algorithm='xgboost', penalty_factor=0.5):
    """
    åˆ›å»ºå¸¦æ³›åŒ–çº¦æŸçš„ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    
    Args:
        penalty_factor: è¿‡æ‹Ÿåˆæƒ©ç½šæƒé‡(0-1)
            - 0: ä¸æƒ©ç½šè¿‡æ‹Ÿåˆï¼Œå•çº¯ä¼˜åŒ–éªŒè¯é›†RMSE
            - 0.5: å¹³è¡¡æ€§èƒ½ä¸æ³›åŒ–
            - 1.0: æ¿€è¿›æƒ©ç½šè¿‡æ‹Ÿåˆ
    """
    
    def objective_xgboost(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 300, 1200),
            'max_depth': trial.suggest_int('max_depth', 3, 8),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
            'gamma': trial.suggest_float('gamma', 0.0, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 10.0, log=True),  # L1
            'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 10.0, log=True), # L2
            'subsample': trial.suggest_float('subsample', 0.6, 0.95),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),
            'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.7, 1.0),
            'n_jobs': -1,
            'random_state': 42
        }
        
        model = XGBRegressor(**params)
        
        # 5-Fold CV with train score
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_results = cross_validate(
            model, X, y, cv=cv,
            scoring={'rmse': 'neg_root_mean_squared_error', 'r2': 'r2'},
            return_train_score=True,
            n_jobs=1
        )
        
        rmse_val = -cv_results['test_rmse'].mean()
        rmse_train = -cv_results['train_rmse'].mean()
        
        # å…³é”®ï¼šæ³›åŒ–çº¦æŸLoss
        overfit_gap = rmse_val - rmse_train
        
        # æœ€ç»ˆç›®æ ‡ = éªŒè¯é›†RMSE + æƒ©ç½šé¡¹
        final_loss = rmse_val + (penalty_factor * max(0, overfit_gap))
        
        # è®°å½•é¢å¤–æŒ‡æ ‡
        trial.set_user_attr('rmse_val', rmse_val)
        trial.set_user_attr('rmse_train', rmse_train)
        trial.set_user_attr('overfit_gap', overfit_gap)
        trial.set_user_attr('overfit_ratio', rmse_train / rmse_val if rmse_val > 0 else 0)
        trial.set_user_attr('r2_val', cv_results['test_r2'].mean())
        
        return final_loss
    
    def objective_catboost(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 300, 1500),
            'depth': trial.suggest_int('depth', 4, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),
            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
            'random_strength': trial.suggest_float('random_strength', 0.0, 2.0),
            'border_count': trial.suggest_int('border_count', 32, 255),
            'verbose': 0,
            'random_seed': 42
        }
        
        model = CatBoostRegressor(**params)
        
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_results = cross_validate(
            model, X, y, cv=cv,
            scoring={'rmse': 'neg_root_mean_squared_error', 'r2': 'r2'},
            return_train_score=True,
            n_jobs=1
        )
        
        rmse_val = -cv_results['test_rmse'].mean()
        rmse_train = -cv_results['train_rmse'].mean()
        overfit_gap = rmse_val - rmse_train
        
        final_loss = rmse_val + (penalty_factor * max(0, overfit_gap))
        
        trial.set_user_attr('rmse_val', rmse_val)
        trial.set_user_attr('rmse_train', rmse_train)
        trial.set_user_attr('overfit_gap', overfit_gap)
        trial.set_user_attr('overfit_ratio', rmse_train / rmse_val if rmse_val > 0 else 0)
        trial.set_user_attr('r2_val', cv_results['test_r2'].mean())
        
        return final_loss
    
    if algorithm == 'xgboost':
        return objective_xgboost
    elif algorithm == 'catboost':
        if not CATBOOST_AVAILABLE:
            raise ValueError("CatBoostæœªå®‰è£…")
        return objective_catboost
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")


def optimize_hyperparameters(data_path, features_path, algorithm='xgboost', 
                            n_trials=100, penalty_factor=0.5, output_dir='models/optuna_results'):
    """æ‰§è¡ŒOptunaä¼˜åŒ–"""
    
    if not OPTUNA_AVAILABLE:
        print("âŒ Optunaæœªå®‰è£…ï¼Œæ— æ³•æ‰§è¡Œä¼˜åŒ–")
        return
    
    print("=" * 80)
    print(f"ğŸ¯ Optunaè¶…å‚æ•°ä¼˜åŒ– - {algorithm.upper()}")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {data_path}")
    print(f"ğŸ”¢ Trialæ•°: {n_trials}")
    print(f"âš–ï¸ è¿‡æ‹Ÿåˆæƒ©ç½šç³»æ•°: {penalty_factor}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path)
    
    with open(features_path, 'r', encoding='utf-8') as f:
        feature_config = json.load(f)
    features = feature_config['selected_features']
    
    df_clean = df.dropna(subset=['hv'])
    X = df_clean[features].copy()
    y = df_clean['hv'].copy()
    
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.fillna(X.median())
    
    print(f"âœ… {len(X)} æ ·æœ¬, {len(features)} ç‰¹å¾")
    
    # åˆ›å»ºä¼˜åŒ–ç›®æ ‡
    print(f"\n[2/5] åˆ›å»ºä¼˜åŒ–ç›®æ ‡ (ç®—æ³•: {algorithm})...")
    objective = create_objective(X, y, algorithm=algorithm, penalty_factor=penalty_factor)
    
    # åˆ›å»ºStudy
    print(f"\n[3/5] å¼€å§‹ä¼˜åŒ– ({n_trials} trials)...")
    study = optuna.create_study(
        direction='minimize',
        study_name=f'hv_{algorithm}_optimization',
        sampler=optuna.samplers.TPESampler(seed=42)
    )
    
    # æ‰§è¡Œä¼˜åŒ–
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    
    # è·å–æœ€ä½³å‚æ•°
    print(f"\n[4/5] ä¼˜åŒ–å®Œæˆï¼")
    best_trial = study.best_trial
    
    print(f"\n   æœ€ä½³Trial #{best_trial.number}:")
    print(f"      Final Loss: {best_trial.value:.4f}")
    print(f"      RMSE (Val): {best_trial.user_attrs['rmse_val']:.4f}")
    print(f"      RMSE (Train): {best_trial.user_attrs['rmse_train']:.4f}")
    print(f"      Overfit Gap: {best_trial.user_attrs['overfit_gap']:.4f}")
    print(f"      Overfit Ratio: {best_trial.user_attrs['overfit_ratio']:.4f}")
    print(f"      RÂ² (Val): {best_trial.user_attrs['r2_val']:.4f}")
    
    print(f"\n   æœ€ä½³å‚æ•°:")
    for key, value in best_trial.params.items():
        print(f"      {key}: {value}")
    
    # ä¿å­˜ç»“æœ
    print(f"\n[5/5] ä¿å­˜ç»“æœ...")
    output_path = Path(output_dir) / algorithm
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æœ€ä½³å‚æ•°
    best_params = {
        'algorithm': algorithm,
        'best_params': best_trial.params,
        'best_metrics': {
            'final_loss': float(best_trial.value),
            'rmse_val': float(best_trial.user_attrs['rmse_val']),
            'rmse_train': float(best_trial.user_attrs['rmse_train']),
            'overfit_gap': float(best_trial.user_attrs['overfit_gap']),
            'overfit_ratio': float(best_trial.user_attrs['overfit_ratio']),
            'r2_val': float(best_trial.user_attrs['r2_val'])
        },
        'optimization_config': {
            'n_trials': n_trials,
            'penalty_factor': penalty_factor
        },
        'timestamp': datetime.now().isoformat()
    }
    
    with open(output_path / f'best_params_{algorithm}.json', 'w', encoding='utf-8') as f:
        json.dump(best_params, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… å‚æ•°ä¿å­˜: {output_path / f'best_params_{algorithm}.json'}")
    
    # ä¿å­˜Studyå¯¹è±¡
    import joblib
    joblib.dump(study, output_path / f'optuna_study_{algorithm}.pkl')
    print(f"   âœ… Studyä¿å­˜: {output_path / f'optuna_study_{algorithm}.pkl'}")
    
    print("\n" + "=" * 80)
    print("âœ… ä¼˜åŒ–å®Œæˆï¼")
    print("=" * 80)
    print(f"\nğŸ“ˆ æ€§èƒ½æå‡é¢„æœŸ:")
    print(f"   å½“å‰åŸºå‡† RÂ²: ~0.816")
    print(f"   ä¼˜åŒ–å RÂ²: ~{best_trial.user_attrs['r2_val']:.3f}")
    print(f"   è¿‡æ‹Ÿåˆæ¯”ç‡: {best_trial.user_attrs['overfit_ratio']:.3f}")
    print("=" * 80)
    
    return best_params


def main():
    parser = argparse.ArgumentParser(description='Optunaè¶…å‚æ•°ä¼˜åŒ–')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data_fixed.csv')
    parser.add_argument('--features', type=str,
                       default='models/selected_features_top30.json')
    parser.add_argument('--algorithm', type=str, 
                       choices=['xgboost', 'catboost', 'lightgbm'],
                       default='xgboost')
    parser.add_argument('--n_trials', type=int, default=100,
                       help='Optunaè¯•éªŒæ¬¡æ•°')
    parser.add_argument('--penalty', type=float, default=0.5,
                       help='è¿‡æ‹Ÿåˆæƒ©ç½šç³»æ•°(0-1)')
    parser.add_argument('--output', type=str,
                       default='models/optuna_results')
    
    args = parser.parse_args()
    
    optimize_hyperparameters(
        data_path=args.data,
        features_path=args.features,
        algorithm=args.algorithm,
        n_trials=args.n_trials,
        penalty_factor=args.penalty,
        output_dir=args.output
    )


if __name__ == "__main__":
    main()
