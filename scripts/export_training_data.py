"""
æ•°æ®å¯¼å‡ºè„šæœ¬ - ä»æ•°æ®åº“æå–å®Œæ•´è®­ç»ƒæ•°æ®

åŠŸèƒ½:
1. JOINæ‰€æœ‰è¡¨ï¼ˆexperiments + compositions + properties + calculated_featuresï¼‰
2. å¯¼å‡ºä¸ºCSVæ ¼å¼
3. ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Šï¼ˆç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ç»Ÿè®¡ï¼‰

ä½¿ç”¨:
    python scripts/export_training_data.py [--output datasets/exported_training_data.csv]
    
ä½œè€…: HEACéªŒè¯æµç¨‹
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.db_models import Experiment, Composition, Property, CalculatedFeature
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker


def export_training_data(db_path='cermet_master_v2.db', output_path='datasets/exported_training_data.csv'):
    """ä»æ•°æ®åº“å¯¼å‡ºå®Œæ•´è®­ç»ƒæ•°æ®"""
    
    print("=" * 80)
    print("ğŸš€ æ•°æ®å¯¼å‡ºè„šæœ¬")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®åº“: {db_path}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("=" * 80)
    
    # è¿æ¥æ•°æ®åº“
    engine = create_engine(f'sqlite:///{db_path}')
    Session = sessionmaker(bind=engine)
    session = Session()
    
    print("\n[1/5] æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“...")
    
    # æŸ¥è¯¢æ‰€æœ‰å®éªŒåŠå…¶å…³è”æ•°æ®
    query = session.query(
        # Experimentè¡¨
        Experiment.id.label('exp_id'),
        Experiment.source_id,
        Experiment.raw_composition,
        Experiment.sinter_temp_c,
        Experiment.grain_size_um,
        Experiment.sinter_method,
        Experiment.load_kgf,
        
        # Compositionè¡¨
        Composition.binder_formula,
        Composition.binder_wt_pct,
        Composition.binder_vol_pct,
        Composition.ceramic_formula,
        Composition.ceramic_wt_pct,
        Composition.ceramic_vol_pct,
        Composition.is_hea,
        Composition.element_count,
        
        # Propertyè¡¨
        Property.hv,
        Property.kic,
        Property.trs,
        Property.youngs_modulus,
        Property.hardness_grade,
        Property.toughness_grade,
        
        # CalculatedFeatureè¡¨
        CalculatedFeature.pred_formation_energy,
        CalculatedFeature.pred_lattice_param,
        CalculatedFeature.pred_magnetic_moment,
        CalculatedFeature.pred_bulk_modulus,
        CalculatedFeature.pred_shear_modulus,
        CalculatedFeature.lattice_mismatch,
        CalculatedFeature.vec_binder,
        CalculatedFeature.mean_atomic_radius,
        CalculatedFeature.binder_density,
        CalculatedFeature.magpie_mean_atomic_mass,
        CalculatedFeature.magpie_std_electronegativity,
        CalculatedFeature.ceramic_magpie_features,
        CalculatedFeature.binder_magpie_features,
        CalculatedFeature.has_matminer,
        CalculatedFeature.has_full_matminer
    ).join(
        Composition, Experiment.id == Composition.exp_id
    ).join(
        Property, Experiment.id == Property.exp_id
    ).outerjoin(  # LEFT JOIN for features (å¯èƒ½ç¼ºå¤±)
        CalculatedFeature, Experiment.id == CalculatedFeature.exp_id
    )
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.read_sql(query.statement, session.bind)
    session.close()
    
    print(f"âœ… æˆåŠŸæŸ¥è¯¢ {len(df)} æ¡è®°å½•")
    
    # å±•å¼€JSONå­—æ®µ
    print("\n[2/5] å±•å¼€JSONç‰¹å¾å­—æ®µ...")
    
    def expand_json_features(df, json_col, prefix):
        """å±•å¼€JSONåˆ—ä¸ºç‹¬ç«‹åˆ—"""
        if json_col not in df.columns:
            return df
        
        expanded_data = []
        for idx, row in df.iterrows():
            if pd.notna(row[json_col]) and row[json_col]:
                try:
                    if isinstance(row[json_col], str):
                        features = json.loads(row[json_col])
                    else:
                        features = row[json_col]
                    expanded_data.append(features)
                except:
                    expanded_data.append({})
            else:
                expanded_data.append({})
        
        if expanded_data and any(expanded_data):
            expanded_df = pd.DataFrame(expanded_data)
            expanded_df.columns = [f"{prefix}_{col}" for col in expanded_df.columns]
            df = pd.concat([df, expanded_df], axis=1)
        
        return df
    
    df = expand_json_features(df, 'ceramic_magpie_features', 'ceramic_magpie')
    df = expand_json_features(df, 'binder_magpie_features', 'binder_magpie')
    
    # åˆ é™¤åŸå§‹JSONåˆ—
    df = df.drop(columns=['ceramic_magpie_features', 'binder_magpie_features'], errors='ignore')
    
    print(f"âœ… å±•å¼€åç‰¹å¾æ•°: {len(df.columns)}")
    
    # ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    print("\n[3/5] ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...")
    
    quality_report = {
        'timestamp': datetime.now().isoformat(),
        'total_records': len(df),
        'total_features': len(df.columns),
        'missing_statistics': {},
        'target_statistics': {},
        'feature_coverage': {}
    }
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing_stats = df.isnull().sum()
    quality_report['missing_statistics'] = {
        col: {
            'missing_count': int(count),
            'missing_percent': float(count / len(df) * 100)
        }
        for col, count in missing_stats.items() if count > 0
    }
    
    # ç›®æ ‡å˜é‡ç»Ÿè®¡
    for target in ['hv', 'kic', 'trs']:
        if target in df.columns:
            valid_data = df[target].dropna()
            quality_report['target_statistics'][target] = {
                'count': int(len(valid_data)),
                'mean': float(valid_data.mean()),
                'std': float(valid_data.std()),
                'min': float(valid_data.min()),
                'max': float(valid_data.max()),
                'missing_count': int(df[target].isnull().sum())
            }
    
    # ç‰¹å¾è¦†ç›–ç‡
    quality_report['feature_coverage'] = {
        'has_proxy_features': int(df['pred_formation_energy'].notna().sum()),
        'has_matminer': int(df['has_matminer'].sum()) if 'has_matminer' in df.columns else 0,
        'has_full_matminer': int(df['has_full_matminer'].sum()) if 'has_full_matminer' in df.columns else 0
    }
    
    # ä¿å­˜æ•°æ®
    print("\n[4/5] ä¿å­˜å¯¼å‡ºæ•°æ®...")
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    df.to_csv(output_path, index=False)
    print(f"âœ… æ•°æ®å·²ä¿å­˜: {output_path}")
    
    # ä¿å­˜è´¨é‡æŠ¥å‘Š
    report_path = output_path.replace('.csv', '_quality_report.json')
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(quality_report, f, indent=2, ensure_ascii=False)
    print(f"âœ… è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # æ‰“å°æ‘˜è¦
    print("\n[5/5] æ•°æ®è´¨é‡æ‘˜è¦")
    print("=" * 80)
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {quality_report['total_records']}")
    print(f"ğŸ“Š æ€»ç‰¹å¾æ•°: {quality_report['total_features']}")
    print(f"\nğŸ¯ ç›®æ ‡å˜é‡ç»Ÿè®¡:")
    for target, stats in quality_report['target_statistics'].items():
        print(f"  {target.upper()}: {stats['count']} æ¡æœ‰æ•ˆæ•°æ® (ç¼ºå¤±: {stats['missing_count']})")
        print(f"    èŒƒå›´: [{stats['min']:.2f}, {stats['max']:.2f}], å‡å€¼: {stats['mean']:.2f}")
    
    print(f"\nğŸ”¬ ç‰¹å¾è¦†ç›–:")
    print(f"  Proxyç‰¹å¾: {quality_report['feature_coverage']['has_proxy_features']} æ¡")
    print(f"  ç®€åŒ–Matminer: {quality_report['feature_coverage']['has_matminer']} æ¡")
    print(f"  å®Œæ•´Matminer: {quality_report['feature_coverage']['has_full_matminer']} æ¡")
    
    print(f"\nâš ï¸  é«˜ç¼ºå¤±ç‡ç‰¹å¾ (>50%):")
    high_missing = {k: v for k, v in quality_report['missing_statistics'].items() 
                    if v['missing_percent'] > 50}
    if high_missing:
        for col, stats in sorted(high_missing.items(), key=lambda x: x[1]['missing_percent'], reverse=True)[:10]:
            print(f"  {col}: {stats['missing_percent']:.1f}%")
    else:
        print("  æ— ")
    
    print("\n" + "=" * 80)
    print("âœ… æ•°æ®å¯¼å‡ºå®Œæˆï¼")
    print("=" * 80)
    
    return df, quality_report


def main():
    parser = argparse.ArgumentParser(description='ä»æ•°æ®åº“å¯¼å‡ºè®­ç»ƒæ•°æ®')
    parser.add_argument('--db', type=str, default='cermet_master_v2.db',
                       help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, 
                       default='datasets/exported_training_data.csv',
                       help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    export_training_data(db_path=args.db, output_path=args.output)


if __name__ == "__main__":
    main()
