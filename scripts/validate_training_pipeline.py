"""
ç³»ç»Ÿæ€§é—®é¢˜éªŒè¯è„šæœ¬

åŠŸèƒ½:
1. æ•°æ®æ³„éœ²æ£€æµ‹
2. æ—¶é—´æ³„éœ²æ£€æµ‹  
3. ç‰¹å¾å…±çº¿æ€§åˆ†æ
4. å¼‚å¸¸å€¼å½±å“åˆ†æ

ä½¿ç”¨:
    python scripts/validate_training_pipeline.py
    
ä½œè€…: HEACéªŒè¯æµç¨‹
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def check_data_leakage(df, target='hv'):
    """æ£€æŸ¥æ•°æ®æ³„éœ²"""
    
    print("\n[1/4] æ•°æ®æ³„éœ²æ£€æµ‹")
    print("=" * 80)
    
    issues = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾ä¸ç›®æ ‡å˜é‡å®Œç¾ç›¸å…³
    target_data = df[target].dropna()
    feature_cols = [col for col in df.columns 
                   if col not in ['hv', 'kic', 'trs', 'youngs_modulus', 'exp_id', 'source_id']]
    
    high_corr_features = []
    for col in feature_cols:
        if col in df.columns and df[col].dtype in ['float64', 'int64']:
            valid_idx = df[col].notna() & target_data.notna()
            if valid_idx.sum() > 10:
                corr = np.corrcoef(df.loc[valid_idx, col], target_data.loc[valid_idx])[0, 1]
                if abs(corr) > 0.95:
                    high_corr_features.append({
                        'feature': col,
                        'correlation': float(corr)
                    })
    
    if high_corr_features:
        print(f"   âš ï¸  å‘ç° {len(high_corr_features)} ä¸ªé«˜ç›¸å…³ç‰¹å¾ï¼ˆ|r| > 0.95ï¼‰:")
        for item in high_corr_features[:10]:
            print(f"      {item['feature']}: r = {item['correlation']:.4f}")
        issues.append({
            'type': 'high_correlation',
            'severity': 'warning',
            'count': len(high_corr_features),
            'features': high_corr_features
        })
    else:
        print(f"   âœ… æœªå‘ç°å®Œç¾ç›¸å…³ç‰¹å¾")
    
    # æ£€æŸ¥ç‰¹å¾åç§°ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡å˜é‡çº¿ç´¢
    suspicious_names = [col for col in feature_cols 
                       if any(keyword in col.lower() 
                             for keyword in ['hv', 'hardness', 'kic', 'toughness', 'trs'])]
    
    if suspicious_names:
        print(f"\n   âš ï¸  å‘ç° {len(suspicious_names)} ä¸ªå¯ç–‘ç‰¹å¾åï¼ˆåŒ…å«ç›®æ ‡å˜é‡å…³é”®è¯ï¼‰:")
        for name in suspicious_names[:10]:
            print(f"      - {name}")
        issues.append({
            'type': 'suspicious_names',
            'severity': 'warning',
            'features': suspicious_names
        })
    else:
        print(f"   âœ… ç‰¹å¾å‘½åæ— å¯ç–‘ä¹‹å¤„")
    
    return issues


def check_feature_multicollinearity(df, threshold=0.9):
    """æ£€æŸ¥ç‰¹å¾å…±çº¿æ€§"""
    
    print("\n[2/4] ç‰¹å¾å…±çº¿æ€§åˆ†æ")
    print("=" * 80)
    
    feature_cols = [col for col in df.columns 
                   if col not in ['hv', 'kic', 'trs', 'exp_id', 'source_id', 'raw_composition']
                   and df[col].dtype in ['float64', 'int64']]
    
    X = df[feature_cols].fillna(df[feature_cols].median())
    
    print(f"   åˆ†æ {len(feature_cols)} ä¸ªç‰¹å¾çš„ç›¸å…³æ€§...")
    
    # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = X.corr().abs()
    
    # æ‰¾åˆ°é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
    high_corr_pairs = []
    for i in range(len(corr_matrix)):
        for j in range(i+1, len(corr_matrix)):
            if corr_matrix.iloc[i, j] > threshold:
                high_corr_pairs.append({
                    'feature1': corr_matrix.index[i],
                    'feature2': corr_matrix.columns[j],
                    'correlation': float(corr_matrix.iloc[i, j])
                })
    
    if high_corr_pairs:
        print(f"   âš ï¸  å‘ç° {len(high_corr_pairs)} å¯¹é«˜åº¦ç›¸å…³ç‰¹å¾ï¼ˆ|r| > {threshold}ï¼‰:")
        for pair in sorted(high_corr_pairs, key=lambda x: x['correlation'], reverse=True)[:15]:
            print(f"      {pair['feature1'][:35]:<35} <-> {pair['feature2'][:35]:<35} r = {pair['correlation']:.4f}")
        
        return {
            'high_corr_count': len(high_corr_pairs),
            'pairs': high_corr_pairs,
            'severity': 'warning' if len(high_corr_pairs) > 50 else 'info'
        }
    else:
        print(f"   âœ… æœªå‘ç°ä¸¥é‡å…±çº¿æ€§é—®é¢˜")
        return {'high_corr_count': 0, 'severity': 'ok'}


def check_outlier_impact(df, target='hv'):
    """æ£€æŸ¥å¼‚å¸¸å€¼å½±å“"""
    
    print("\n[3/4] å¼‚å¸¸å€¼å½±å“åˆ†æ")
    print("=" * 80)
    
    target_data = df[target].dropna()
    
    # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
    Q1 = target_data.quantile(0.25)
    Q3 = target_data.quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = target_data[(target_data < lower_bound) | (target_data > upper_bound)]
    outlier_ratio = len(outliers) / len(target_data) * 100
    
    print(f"   ç›®æ ‡å˜é‡ {target.upper()}:")
    print(f"      èŒƒå›´: [{target_data.min():.1f}, {target_data.max():.1f}]")
    print(f"      IQRèŒƒå›´: [{lower_bound:.1f}, {upper_bound:.1f}]")
    print(f"      å¼‚å¸¸å€¼æ•°é‡: {len(outliers)} ({outlier_ratio:.2f}%)")
    
    if outlier_ratio > 5:
        print(f"      âš ï¸  å¼‚å¸¸å€¼æ¯”ä¾‹è¾ƒé«˜ï¼Œå¯èƒ½å½±å“æ¨¡å‹è®­ç»ƒ")
        severity = 'warning'
    else:
        print(f"      âœ… å¼‚å¸¸å€¼æ¯”ä¾‹æ­£å¸¸")
        severity = 'ok'
    
    # æ£€æŸ¥æç«¯å€¼
    extreme_values = target_data[target_data < target_data.mean() - 3*target_data.std()]
    if len(extreme_values) > 0:
        print(f"      âš ï¸  å‘ç° {len(extreme_values)} ä¸ªæç«¯ä½å€¼ï¼ˆ< Î¼-3Ïƒï¼‰")
        print(f"         å€¼: {extreme_values.values[:5]}")
    
    return {
        'outlier_count': int(len(outliers)),
        'outlier_ratio': float(outlier_ratio),
        'lower_bound': float(lower_bound),
        'upper_bound': float(upper_bound),
        'severity': severity,
        'extreme_values': extreme_values.tolist()
    }


def check_data_splitting_strategy(df):
    """æ£€æŸ¥æ•°æ®åˆ†å‰²ç­–ç•¥"""
    
    print("\n[4/4] æ•°æ®åˆ†å‰²ç­–ç•¥æ£€æŸ¥")
    print("=" * 80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ä¿¡æ¯
    has_timestamp = 'created_at' in df.columns or 'updated_at' in df.columns
    
    if has_timestamp:
        print(f"   âš ï¸  æ•°æ®åŒ…å«æ—¶é—´æˆ³ä¿¡æ¯")
        print(f"      å»ºè®®ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²éªŒè¯ï¼ˆTimeSeriesSplitï¼‰")
        severity = 'warning'
    else:
        print(f"   âœ… æ— æ—¶é—´åºåˆ—ä¿¡æ¯ï¼Œå¯ä½¿ç”¨éšæœºåˆ†å‰²")
        severity = 'ok'
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ ·æœ¬
    key_cols = ['ceramic_formula', 'binder_formula', 'sinter_temp_c', 'grain_size_um']
    available_keys = [col for col in key_cols if col in df.columns]
    
    if available_keys:
        duplicates = df.duplicated(subset=available_keys, keep=False)
        dup_count = duplicates.sum()
        
        if dup_count > 0:
            print(f"   âš ï¸  å‘ç° {dup_count} æ¡å¯èƒ½çš„é‡å¤å®éªŒï¼ˆåŸºäºå·¥è‰ºå‚æ•°ï¼‰")
            print(f"      å»ºè®®åœ¨åˆ†å‰²æ—¶è€ƒè™‘å»é‡")
            severity = 'warning'
        else:
            print(f"   âœ… æœªå‘ç°æ˜æ˜¾é‡å¤å®éªŒ")
    
    return {
        'has_timestamp': has_timestamp,
        'severity': severity
    }


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯è®­ç»ƒæµç¨‹çš„ç³»ç»Ÿæ€§é—®é¢˜')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data.csv',
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--target', type=str, default='hv',
                       choices=['hv', 'kic', 'trs'],
                       help='ç›®æ ‡å˜é‡')
    parser.add_argument('--output', type=str,
                       default='models/systematic_validation_report.json',
                       help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ” ç³»ç»Ÿæ€§é—®é¢˜éªŒè¯è„šæœ¬")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {args.data}")
    print(f"ğŸ¯ ç›®æ ‡: {args.target.upper()}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(args.data)
    print(f"\nâœ… åŠ è½½ {len(df)} æ¡è®°å½•")
    
    # æ‰§è¡Œå„é¡¹æ£€æŸ¥
    report = {
        'timestamp': datetime.now().isoformat(),
        'data_path': args.data,
        'target': args.target,
        'sample_count': len(df)
    }
    
    report['data_leakage'] = check_data_leakage(df, target=args.target)
    report['multicollinearity'] = check_feature_multicollinearity(df, threshold=0.9)
    report['outlier_impact'] = check_outlier_impact(df, target=args.target)
    report['data_splitting'] = check_data_splitting_strategy(df)
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 80)
    print("ğŸ“Š éªŒè¯æ‘˜è¦")
    print("=" * 80)
    
    # ç»Ÿè®¡é—®é¢˜
    total_warnings = 0
    if report['data_leakage']:
        total_warnings += len(report['data_leakage'])
    if report['multicollinearity']['severity'] in ['warning']:
        total_warnings += 1
    if report['outlier_impact']['severity'] == 'warning':
        total_warnings += 1
    if report['data_splitting']['severity'] == 'warning':
        total_warnings += 1
    
    print(f"æ£€æµ‹åˆ° {total_warnings} ä¸ªæ½œåœ¨é—®é¢˜")
    
    if total_warnings == 0:
        print("âœ… è®­ç»ƒæµç¨‹ç³»ç»Ÿæ€§æ£€æŸ¥é€šè¿‡ï¼")
    else:
        print("âš ï¸  å»ºè®®å…³æ³¨ä»¥ä¸Šè­¦å‘Šä¿¡æ¯")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {output_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
