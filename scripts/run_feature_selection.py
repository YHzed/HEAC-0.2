"""
ç‰¹å¾é€‰æ‹©è„šæœ¬ - å‘½ä»¤è¡Œç‰ˆGBFS

åŠŸèƒ½:
1. è¯»å–å¯¼å‡ºçš„è®­ç»ƒæ•°æ®
2. ç”Ÿæˆå¤åˆç‰¹å¾ï¼ˆceramic-binderäº¤äº’ï¼‰
3. æ‰§è¡ŒGBFSåˆ†å±‚èšç±»ç‰¹å¾é€‰æ‹©
4. ä¿å­˜é€‰å®šçš„ç‰¹å¾åˆ—è¡¨

ä½¿ç”¨:
    python scripts/run_feature_selection.py --target hv [--threshold 0.7]
    
ä½œè€…: HEACéªŒè¯æµç¨‹
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.stats import spearmanr
from scipy.spatial.distance import squareform

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def generate_composite_features(df):
    """ç”Ÿæˆceramic-binderå¤åˆç‰¹å¾"""
    
    print("\n[2/5] ç”Ÿæˆå¤åˆç‰¹å¾...")
    
    df_new = df.copy()
    new_features = []
    
    # å®šä¹‰ceramicå’Œbinderç‰¹å¾é…å¯¹
    ceramic_features = [col for col in df.columns if col.startswith('ceramic_magpie_')]
    binder_features = [col for col in df.columns if col.startswith('binder_magpie_')]
    
    # æå–å…±åŒçš„ç‰¹å¾åï¼ˆå»æ‰å‰ç¼€ï¼‰
    ceramic_base = {col.replace('ceramic_magpie_', ''): col for col in ceramic_features}
    binder_base = {col.replace('binder_magpie_', ''): col for col in binder_features}
    
    common_features = set(ceramic_base.keys()) & set(binder_base.keys())
    
    print(f"   å‘ç° {len(common_features)} ä¸ªå…±åŒç‰¹å¾å¯ç”Ÿæˆå¤åˆç‰¹å¾")
    
    # 1. åŠ æƒå¹³å‡ç‰¹å¾ (Weighted Average)
    for feat in common_features:
        ceramic_col = ceramic_base[feat]
        binder_col = binder_base[feat]
        
        if ceramic_col in df.columns and binder_col in df.columns:
            # ä½¿ç”¨ä½“ç§¯ç™¾åˆ†æ¯”ä½œä¸ºæƒé‡
            if 'ceramic_vol_pct' in df.columns and 'binder_vol_pct' in df.columns:
                df_new[f'Mean_{feat}'] = (
                    df[ceramic_col] * df['ceramic_vol_pct'] / 100 +
                    df[binder_col] * df['binder_vol_pct'] / 100
                )
                new_features.append(f'Mean_{feat}')
    
    # 2. å·®å¼‚ç‰¹å¾ (Difference) - ç•Œé¢åº”åŠ›ã€å¤±é…
    for feat in common_features:
        ceramic_col = ceramic_base[feat]
        binder_col = binder_base[feat]
        
        if ceramic_col in df.columns and binder_col in df.columns:
            df_new[f'Diff_{feat}'] = abs(df[ceramic_col] - df[binder_col])
            new_features.append(f'Diff_{feat}')
    
    # 3. æ¯”å€¼ç‰¹å¾ (Ratio) - ç›¸å¯¹å…³ç³»
    for feat in common_features:
        ceramic_col = ceramic_base[feat]
        binder_col = binder_base[feat]
        
        if ceramic_col in df.columns and binder_col in df.columns:
            # é¿å…é™¤é›¶
            df_new[f'Ratio_{feat}'] = df[ceramic_col] / (df[binder_col] + 1e-6)
            new_features.append(f'Ratio_{feat}')
    
    # 4. ç•Œé¢ç‰¹å¾ (Interface)
    # ç•Œé¢å¤æ‚åº¦ = Ceramic Vol% * Binder Vol%
    if 'ceramic_vol_pct' in df.columns and 'binder_vol_pct' in df.columns:
        df_new['Interface_Complexity'] = (
            df['ceramic_vol_pct'] * df['binder_vol_pct'] / 10000
        )
        new_features.append('Interface_Complexity')
    
    # å¹³å‡è‡ªç”±ç¨‹ âˆ Grain Size / Binder Vol%
    if 'grain_size_um' in df.columns and 'binder_vol_pct' in df.columns:
        df_new['Mean_Free_Path'] = df['grain_size_um'] / (df['binder_vol_pct'] + 1)
        new_features.append('Mean_Free_Path')
    
    print(f"   âœ… ç”Ÿæˆ {len(new_features)} ä¸ªå¤åˆç‰¹å¾")
    
    return df_new, new_features


def perform_gbfs_clustering(X, y, threshold=0.7, critical_features=None):
    """æ‰§è¡ŒGBFSåˆ†å±‚èšç±»ç‰¹å¾é€‰æ‹©"""
    
    print("\n[3/5] æ‰§è¡ŒGBFSåˆ†å±‚èšç±»...")
    
    if critical_features is None:
        critical_features = []
    
    # å…ˆæ¸…ç†æ•°æ®ä¸­çš„æ— ç©·å€¼å’ŒNaN
    X_clean = X.copy()
    X_clean = X_clean.replace([np.inf, -np.inf], np.nan)
    X_clean = X_clean.fillna(X_clean.median())
    
    # è®¡ç®—Spearmanç›¸å…³æ€§çŸ©é˜µ
    print("   è®¡ç®—Spearmanç›¸å…³æ€§çŸ©é˜µ...")
    try:
        corr_matrix, _ = spearmanr(X_clean)
    except Exception as e:
        print(f"   âš ï¸  Spearmanè®¡ç®—å¤±è´¥: {e}")
        print(f"   ä½¿ç”¨Pearsonç›¸å…³ç³»æ•°æ›¿ä»£...")
        corr_matrix = X_clean.corr().values
    
    # ç¡®ä¿æ˜¯numpyæ•°ç»„
    if not isinstance(corr_matrix, np.ndarray):
        corr_matrix = np.array(corr_matrix)
    
    # å¦‚æœåªæœ‰ä¸€åˆ—ï¼Œcorr_matrixæ˜¯æ ‡é‡ï¼Œå¤„ç†ç‰¹æ®Šæƒ…å†µ
    if corr_matrix.ndim == 0:
        corr_matrix = np.array([[1.0]])
    
    # å¤„ç†NaNå€¼ï¼ˆç”¨0æ›¿æ¢ï¼‰
    corr_matrix = np.nan_to_num(corr_matrix, nan=0.0, posinf=1.0, neginf=-1.0)
    
    corr_matrix = np.abs(corr_matrix)  # ä½¿ç”¨ç»å¯¹å€¼
    
    # è½¬æ¢ä¸ºè·ç¦»çŸ©é˜µï¼Œç¡®ä¿å¯¹ç§°æ€§å’Œæœ‰é™æ€§
    distance_matrix = 1 - corr_matrix
    # å¼ºåˆ¶å¯¹ç§°ï¼ˆå–ä¸Šä¸‰è§’å’Œä¸‹ä¸‰è§’çš„å¹³å‡ï¼‰
    distance_matrix = (distance_matrix + distance_matrix.T) / 2
    np.fill_diagonal(distance_matrix, 0)
    
    # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æœ‰é™çš„
    distance_matrix = np.nan_to_num(distance_matrix, nan=1.0, posinf=1.0, neginf=0.0)
    
    # æ‰§è¡Œåˆ†å±‚èšç±»
    print("   æ‰§è¡ŒWard's Linkageèšç±»...")
    try:
        condensed_dist = squareform(distance_matrix, checks=False)
        linkage_matrix = linkage(condensed_dist, method='ward')
    except Exception as e:
        print(f"   âš ï¸  èšç±»å¤±è´¥: {e}")
        # é™çº§æ–¹æ¡ˆï¼šéšæœºé€‰æ‹©ç‰¹å¾
        print(f"   ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šåŸºäºç›¸å…³æ€§æ’åºé€‰æ‹©ç‰¹å¾...")
        n_features = min(50, len(X.columns))  # æœ€å¤šé€‰50ä¸ª
        selected_features = critical_features.copy()
        
        # è®¡ç®—ä¸ç›®æ ‡å˜é‡çš„ç›¸å…³æ€§
        correlations = []
        for col in X.columns:
            if col not in selected_features:
                corr, _ = spearmanr(X[col], y)
                correlations.append((col, abs(corr)))
        
        # æŒ‰ç›¸å…³æ€§æ’åºï¼Œé€‰æ‹©topç‰¹å¾
        correlations.sort(key=lambda x: x[1], reverse=True)
        for col, _ in correlations[:n_features - len(selected_features)]:
            selected_features.append(col)
        
        return selected_features, None, None
    
    # åˆ‡å‰²æ ‘çŠ¶å›¾
    cluster_labels = fcluster(linkage_matrix, t=threshold, criterion='distance')
    
    print(f"   èšç±»é˜ˆå€¼: {threshold}")
    print(f"   ç”Ÿæˆèšç±»æ•°: {len(np.unique(cluster_labels))}")
    
    # ä»æ¯ä¸ªèšç±»ä¸­é€‰æ‹©ä»£è¡¨ç‰¹å¾
    selected_features = []
    feature_names = X.columns.tolist()
    
    for cluster_id in np.unique(cluster_labels):
        cluster_members = [feature_names[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
        
        # ä¼˜å…ˆé€‰æ‹©å…³é”®ç‰©ç†ç‰¹å¾
        critical_in_cluster = [f for f in cluster_members if f in critical_features]
        if critical_in_cluster:
            selected_features.append(critical_in_cluster[0])
            continue
        
        # å¦åˆ™é€‰æ‹©ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æœ€é«˜çš„
        cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]
        cluster_X = X.iloc[:, cluster_indices]
        
        correlations = []
        for col in cluster_X.columns:
            corr, _ = spearmanr(cluster_X[col], y)
            correlations.append(abs(corr))
        
        best_idx = np.argmax(correlations)
        selected_features.append(cluster_members[best_idx])
    
    print(f"   âœ… ä» {len(feature_names)} ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº† {len(selected_features)} ä¸ª")
    
    return selected_features, cluster_labels, linkage_matrix


def main():
    parser = argparse.ArgumentParser(description='GBFSç‰¹å¾é€‰æ‹©')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data.csv',
                       help='è¾“å…¥æ•°æ®è·¯å¾„')
    parser.add_argument('--target', type=str, default='hv',
                       choices=['hv', 'kic', 'trs'],
                       help='ç›®æ ‡å˜é‡')
    parser.add_argument('--threshold', type=float, default=0.7,
                       help='èšç±»è·ç¦»é˜ˆå€¼ï¼ˆè¶Šä½ä¿ç•™ç‰¹å¾è¶Šå¤šï¼‰')
    parser.add_argument('--output', type=str,
                       default='models/selected_features.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸŒ³ GBFSç‰¹å¾é€‰æ‹©è„šæœ¬")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {args.data}")
    print(f"ğŸ¯ ç›®æ ‡: {args.target.upper()}")
    print(f"ğŸ”§ èšç±»é˜ˆå€¼: {args.threshold}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®...")
    df = pd.read_csv(args.data)
    print(f"âœ… åŠ è½½ {len(df)} æ¡è®°å½•ï¼Œ{len(df.columns)} ä¸ªå­—æ®µ")
    
    # ç”Ÿæˆå¤åˆç‰¹å¾
    df_with_composite, new_features = generate_composite_features(df)
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n[4/5] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    
    # åˆ é™¤ç›®æ ‡å˜é‡ä¸ºç©ºçš„è¡Œ
    df_clean = df_with_composite.dropna(subset=[args.target])
    print(f"   ç›®æ ‡å˜é‡ {args.target}: {len(df_clean)} æ¡æœ‰æ•ˆè®°å½•")
    
    # é€‰æ‹©ç‰¹å¾åˆ—
    exclude_cols = [
        'exp_id', 'source_id', 'raw_composition', 'sinter_method',
        'binder_formula', 'ceramic_formula', 'hardness_grade', 'toughness_grade',
        'hv', 'kic', 'trs', 'youngs_modulus',  # ç›®æ ‡å˜é‡
        'has_matminer', 'has_full_matminer', 'created_at', 'updated_at',
        'is_hea', 'element_count'  # åˆ†ç±»å˜é‡
    ]
    
    feature_cols = [col for col in df_clean.columns 
                   if col not in exclude_cols and df_clean[col].dtype in ['float64', 'int64']]
    
    print(f"   å€™é€‰ç‰¹å¾æ•°: {len(feature_cols)}")
    
    # å¡«å……ç¼ºå¤±å€¼
    X = df_clean[feature_cols].copy()
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.fillna(X.median())
    
    y = df_clean[args.target].copy()
    
    # å®šä¹‰å…³é”®ç‰©ç†ç‰¹å¾ï¼ˆå§‹ç»ˆä¿ç•™ï¼‰
    critical_physics_features = [
        'pred_formation_energy',
        'pred_lattice_param',
        'lattice_mismatch',
        'pred_magnetic_moment',
        'binder_vol_pct',
        'grain_size_um',
        'sinter_temp_c'
    ]
    
    critical_features = [f for f in critical_physics_features if f in feature_cols]
    
    # æ‰§è¡ŒGBFS
    selected_features, cluster_labels, linkage_matrix = perform_gbfs_clustering(
        X, y, threshold=args.threshold, critical_features=critical_features
    )
    
    # ä¿å­˜ç»“æœ
    print("\n[5/5] ä¿å­˜ç»“æœ...")
    
    output_data = {
        'target': args.target,
        'threshold': args.threshold,
        'total_features': len(feature_cols),
        'selected_features': selected_features,
        'selected_count': len(selected_features),
        'critical_features': critical_features,
        'composite_features': new_features,
        'timestamp': pd.Timestamp.now().isoformat()
    }
    
    output_path = Path(args.output)
    output_dir = output_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜: {output_path}")
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“Š ç‰¹å¾é€‰æ‹©æ‘˜è¦")
    print("=" * 80)
    print(f"åŸå§‹ç‰¹å¾æ•°: {len(feature_cols)}")
    print(f"å¤åˆç‰¹å¾æ•°: {len(new_features)}")
    print(f"é€‰å®šç‰¹å¾æ•°: {len(selected_features)}")
    print(f"å‹ç¼©æ¯”: {len(selected_features)/len(feature_cols)*100:.1f}%")
    
    print(f"\nğŸŒŸ é€‰å®šçš„å…³é”®ç‰¹å¾ï¼ˆå‰20ä¸ªï¼‰:")
    for i, feat in enumerate(selected_features[:20], 1):
        is_critical = "â­" if feat in critical_features else "  "
        is_composite = "ğŸ§¬" if feat in new_features else "  "
        print(f"   {i:2d}. {is_critical} {is_composite} {feat}")
    
    if len(selected_features) > 20:
        print(f"   ... è¿˜æœ‰ {len(selected_features)-20} ä¸ªç‰¹å¾")
    
    print("\n" + "=" * 80)
    print("âœ… ç‰¹å¾é€‰æ‹©å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()
