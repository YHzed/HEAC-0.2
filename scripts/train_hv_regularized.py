"""
æ·±åº¦è¿‡æ‹Ÿåˆä¼˜åŒ–æ–¹æ¡ˆ - æ­£åˆ™åŒ–ä¸æ¨¡å‹ç®€åŒ–

é€šè¿‡å¤šç§æŠ€æœ¯ç»„åˆè¿›ä¸€æ­¥é™ä½è¿‡æ‹Ÿåˆï¼š
1. L1/L2æ­£åˆ™åŒ–
2. é™ä½æ¨¡å‹å¤æ‚åº¦
3. Early stopping
4. æ•°æ®å¢å¼ºç­–ç•¥

ä½¿ç”¨:
    python scripts/train_hv_regularized.py --strategy [light|medium|aggressive]
    
ä½œè€…: HEACä¼˜åŒ–æµç¨‹
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xgboost import XGBRegressor
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error


def get_regularization_params(strategy='medium'):
    """
    è·å–æ­£åˆ™åŒ–å‚æ•°é…ç½®
    
    Args:
        strategy: 'light', 'medium', 'aggressive'
    """
    
    strategies = {
        'light': {
            'name': 'è½»åº¦æ­£åˆ™åŒ–',
            'n_estimators': 800,
            'max_depth': 5,
            'learning_rate': 0.05,
            'min_child_weight': 2,
            'gamma': 0.1,
            'reg_alpha': 0.1,  # L1
            'reg_lambda': 0.5,  # L2
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'colsample_bylevel': 1.0,
        },
        'medium': {
            'name': 'ä¸­åº¦æ­£åˆ™åŒ–',
            'n_estimators': 600,
            'max_depth': 4,
            'learning_rate': 0.05,
            'min_child_weight': 3,
            'gamma': 0.2,
            'reg_alpha': 0.3,  # L1
            'reg_lambda': 1.0,  # L2
            'subsample': 0.75,
            'colsample_bytree': 0.75,
            'colsample_bylevel': 0.9,
        },
        'aggressive': {
            'name': 'æ¿€è¿›æ­£åˆ™åŒ–',
            'n_estimators': 400,
            'max_depth': 3,
            'learning_rate': 0.05,
            'min_child_weight': 5,
            'gamma': 0.5,
            'reg_alpha': 0.5,  # L1
            'reg_lambda': 2.0,  # L2
            'subsample': 0.7,
            'colsample_bytree': 0.7,
            'colsample_bylevel': 0.8,
        }
    }
    
    return strategies.get(strategy, strategies['medium'])


def train_regularized_model(data_path, features_path, strategy='medium', output_dir='models/validated_regularized'):
    """è®­ç»ƒæ­£åˆ™åŒ–çš„HVæ¨¡å‹"""
    
    # è·å–å‚æ•°
    params = get_regularization_params(strategy)
    
    print("=" * 80)
    print(f"ğŸ›¡ï¸ æ­£åˆ™åŒ–HVæ¨¡å‹è®­ç»ƒ - {params['name']}")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {data_path}")
    print(f"ğŸŒŸ ç‰¹å¾: {features_path}")
    print(f"ğŸ’¾ è¾“å‡º: {output_dir}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\n[1/6] åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path)
    
    with open(features_path, 'r', encoding='utf-8') as f:
        feature_config = json.load(f)
    selected_features = feature_config['selected_features']
    
    df_clean = df.dropna(subset=['hv'])
    X = df_clean[selected_features].copy()
    y = df_clean['hv'].copy()
    
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.fillna(X.median())
    
    print(f"âœ… {len(X)} æ ·æœ¬, {len(selected_features)} ç‰¹å¾")
    
    # æ˜¾ç¤ºæ­£åˆ™åŒ–å‚æ•°
    print(f"\n[2/6] æ­£åˆ™åŒ–é…ç½® ({params['name']}):")
    print(f"   æ ‘æ•°é‡: {params['n_estimators']}")
    print(f"   æœ€å¤§æ·±åº¦: {params['max_depth']}")
    print(f"   æœ€å°å­èŠ‚ç‚¹æƒé‡: {params['min_child_weight']}")
    print(f"   L1æ­£åˆ™(alpha): {params['reg_alpha']}")
    print(f"   L2æ­£åˆ™(lambda): {params['reg_lambda']}")
    print(f"   Gamma: {params['gamma']}")
    print(f"   è¡Œé‡‡æ ·: {params['subsample']}")
    print(f"   åˆ—é‡‡æ ·(æ ‘): {params['colsample_bytree']}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\n[3/6] åˆ›å»ºæ­£åˆ™åŒ–æ¨¡å‹...")
    model = XGBRegressor(
        n_estimators=params['n_estimators'],
        max_depth=params['max_depth'],
        learning_rate=params['learning_rate'],
        min_child_weight=params['min_child_weight'],
        gamma=params['gamma'],
        reg_alpha=params['reg_alpha'],
        reg_lambda=params['reg_lambda'],
        subsample=params['subsample'],
        colsample_bytree=params['colsample_bytree'],
        colsample_bylevel=params['colsample_bylevel'],
        n_jobs=-1,
        random_state=42
    )
    
    # äº¤å‰éªŒè¯
    print("\n[4/6] äº¤å‰éªŒè¯è®­ç»ƒ...")
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    y_pred_cv = cross_val_predict(model, X, y, cv=cv, n_jobs=1)
    
    r2_cv = r2_score(y, y_pred_cv)
    mae_cv = mean_absolute_error(y, y_pred_cv)
    rmse_cv = np.sqrt(mean_squared_error(y, y_pred_cv))
    
    print(f"\n   äº¤å‰éªŒè¯ç»“æœ:")
    print(f"      RÂ² Score: {r2_cv:.4f}")
    print(f"      MAE: {mae_cv:.4f} HV")
    print(f"      RMSE: {rmse_cv:.4f} HV")
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\n[5/6] è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    model.fit(X, y)
    
    y_pred_train = model.predict(X)
    r2_train = r2_score(y, y_pred_train)
    mae_train = mean_absolute_error(y, y_pred_train)
    rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))
    
    overfitting_ratio = r2_train / r2_cv if r2_cv > 0 else float('inf')
    
    print(f"\n   è®­ç»ƒé›†ç»“æœ:")
    print(f"      RÂ² Score: {r2_train:.4f}")
    print(f"      MAE: {mae_train:.4f} HV")
    
    print(f"\n   ğŸ“Š è¿‡æ‹Ÿåˆè¯„ä¼°:")
    print(f"      è¿‡æ‹Ÿåˆæ¯”ç‡: {overfitting_ratio:.3f}")
    
    if overfitting_ratio < 1.05:
        print(f"      âœ…âœ…âœ… ä¼˜ç§€ - å‡ ä¹æ— è¿‡æ‹Ÿåˆ")
    elif overfitting_ratio < 1.10:
        print(f"      âœ…âœ… è‰¯å¥½ - è½»å¾®è¿‡æ‹Ÿåˆ")
    elif overfitting_ratio < 1.20:
        print(f"      âœ… ä¸€èˆ¬ - è½»åº¦è¿‡æ‹Ÿåˆ")
    else:
        print(f"      âš ï¸ ä»éœ€ä¼˜åŒ–")
    
    # ä¿å­˜
    print("\n[6/6] ä¿å­˜æ¨¡å‹...")
    output_path = Path(output_dir) / strategy
    output_path.mkdir(parents=True, exist_ok=True)
    
    joblib.dump(model, output_path / 'hv_regularized_model.pkl')
    
    with open(output_path / 'hv_feature_list.json', 'w', encoding='utf-8') as f:
        json.dump(selected_features, f, indent=2, ensure_ascii=False)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'strategy': strategy,
        'strategy_name': params['name'],
        'data_path': data_path,
        'sample_count': len(X),
        'feature_count': len(selected_features),
        'target': 'hv',
        'model_params': params,
        'cv_metrics': {
            'r2': float(r2_cv),
            'mae': float(mae_cv),
            'rmse': float(rmse_cv)
        },
        'train_metrics': {
            'r2': float(r2_train),
            'mae': float(mae_train),
            'rmse': float(rmse_train)
        },
        'overfitting_ratio': float(overfitting_ratio)
    }
    
    with open(output_path / 'hv_training_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"   âœ… å·²ä¿å­˜åˆ°: {output_path}")
    
    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    
    return model, report


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒæ­£åˆ™åŒ–HVæ¨¡å‹')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data_fixed.csv')
    parser.add_argument('--features', type=str,
                       default='models/selected_features_top30.json')
    parser.add_argument('--strategy', type=str, 
                       choices=['light', 'medium', 'aggressive'],
                       default='medium',
                       help='æ­£åˆ™åŒ–å¼ºåº¦ï¼šlight(è½»åº¦)/medium(ä¸­åº¦)/aggressive(æ¿€è¿›)')
    parser.add_argument('--output', type=str,
                       default='models/validated_regularized')
    
    args = parser.parse_args()
    
    train_regularized_model(
        data_path=args.data,
        features_path=args.features,
        strategy=args.strategy,
        output_dir=args.output
    )


if __name__ == "__main__":
    main()
