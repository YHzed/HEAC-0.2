"""
ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°è®­ç»ƒæœ€ç»ˆHVæ¨¡å‹

ä½¿ç”¨:
    python scripts/train_hv_final_optimized.py
    
ä½œè€…: HEACé«˜è´¨é‡ä¼˜åŒ–
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import json
import joblib
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xgboost import XGBRegressor
from sklearn.model_selection import cross_validate, cross_val_predict, KFold
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error


def train_final_optimized_model(data_path, features_path, params_path, output_dir='models/high_quality'):
    """ä½¿ç”¨Optunaä¼˜åŒ–å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
    
    print("=" * 80)
    print("ğŸ† é«˜è´¨é‡HVæ¨¡å‹è®­ç»ƒ - Optunaä¼˜åŒ–ç‰ˆ")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {data_path}")
    print(f"âš™ï¸ å‚æ•°: {params_path}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path)
    
    with open(features_path, 'r', encoding='utf-8') as f:
        feature_config = json.load(f)
    features = feature_config['selected_features']
    
    df_clean = df.dropna(subset=['hv'])
    X = df_clean[features].copy()
    y = df_clean['hv'].copy()
    
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.fillna(X.median())
    
    print(f"âœ… {len(X)} æ ·æœ¬, {len(features)} ç‰¹å¾")
    
    # åŠ è½½Optunaä¼˜åŒ–å‚æ•°
    print("\n[2/5] åŠ è½½Optunaä¼˜åŒ–å‚æ•°...")
    with open(params_path, 'r', encoding='utf-8') as f:
        optuna_results = json.load(f)
    
    best_params = optuna_results['best_params']
    print(f"   Optunaæœ€ä½³å‚æ•°:")
    for key, value in best_params.items():
        print(f"      {key}: {value}")
    
    print(f"\n   é¢„æœŸæ€§èƒ½:")
    print(f"      RMSE (Val): {optuna_results['best_metrics']['rmse_val']:.2f}")
    print(f"      RÂ² (Val): {optuna_results['best_metrics']['r2_val']:.4f}")
    print(f"      è¿‡æ‹Ÿåˆæ¯”ç‡: {optuna_results['best_metrics']['overfit_ratio']:.3f}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\n[3/5] åˆ›å»ºä¼˜åŒ–æ¨¡å‹...")
    model = XGBRegressor(**best_params)
    
    # äº¤å‰éªŒè¯è¯„ä¼°
    print("\n[4/5] äº¤å‰éªŒè¯è¯„ä¼°...")
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    
    # è¯¦ç»†CVè¯„ä¼°
    cv_results = cross_validate(
        model, X, y, cv=cv,
        scoring={'r2': 'r2', 'mae': 'neg_mean_absolute_error', 
                 'rmse': 'neg_root_mean_squared_error'},
        return_train_score=True,
        n_jobs=1
    )
    
    # CVé¢„æµ‹
    y_pred_cv = cross_val_predict(model, X, y, cv=cv, n_jobs=1)
    
    # CVæŒ‡æ ‡
    r2_cv = r2_score(y, y_pred_cv)
    mae_cv = mean_absolute_error(y, y_pred_cv)
    rmse_cv = np.sqrt(mean_squared_error(y, y_pred_cv))
    
    # è®­ç»ƒé›†æŒ‡æ ‡
    r2_train = cv_results['train_r2'].mean()
    mae_train = -cv_results['train_mae'].mean()
    rmse_train = -cv_results['train_rmse'].mean()
    
    overfit_ratio = r2_train / r2_cv if r2_cv > 0 else float('inf')
    
    print(f"\n   äº¤å‰éªŒè¯ç»“æœ:")
    print(f"      RÂ² (CV): {r2_cv:.4f}")
    print(f"      MAE (CV): {mae_cv:.2f} HV")
    print(f"      RMSE (CV): {rmse_cv:.2f} HV")
    
    print(f"\n   è®­ç»ƒé›†ç»“æœ:")
    print(f"      RÂ² (Train): {r2_train:.4f}")
    print(f"      MAE (Train): {mae_train:.2f} HV")
    print(f"      RMSE (Train): {rmse_train:.2f} HV")
    
    print(f"\n   è¿‡æ‹Ÿåˆè¯„ä¼°:")
    print(f"      è¿‡æ‹Ÿåˆæ¯”ç‡: {overfit_ratio:.3f}")
    
    if overfit_ratio < 1.05:
        status = "âœ…âœ…âœ… ä¼˜ç§€"
    elif overfit_ratio < 1.10:
        status = "âœ…âœ… è‰¯å¥½"
    elif overfit_ratio < 1.15:
        status = "âœ… ä¸€èˆ¬"
    else:
        status = "âš ï¸ éœ€è¿›ä¸€æ­¥ä¼˜åŒ–"
    print(f"      {status}")
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\n[5/5] è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    model.fit(X, y)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\n   ç‰¹å¾é‡è¦æ€§ (Top 10):")
    for idx, row in feature_importance.head(10).iterrows():
        print(f"      {row['feature']:<40} {row['importance']:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    joblib.dump(model, output_path / 'hv_optimized_model.pkl')
    
    with open(output_path / 'hv_feature_list.json', 'w', encoding='utf-8') as f:
        json.dump(features, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'model_type': 'XGBoost (Optuna Optimized)',
        'optimization_method': 'Optuna TPESampler',
        'n_trials': optuna_results['optimization_config']['n_trials'],
        'penalty_factor': optuna_results['optimization_config']['penalty_factor'],
        'sample_count': len(X),
        'feature_count': len(features),
        'best_params': best_params,
        'cv_metrics': {
            'r2': float(r2_cv),
            'mae': float(mae_cv),
            'rmse': float(rmse_cv)
        },
        'train_metrics': {
            'r2': float(r2_train),
            'mae': float(mae_train),
            'rmse': float(rmse_train)
        },
        'overfitting_ratio': float(overfit_ratio),
        'feature_importance': feature_importance.to_dict('records')
    }
    
    with open(output_path / 'hv_training_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"\n   âœ… æ¨¡å‹ä¿å­˜: {output_path / 'hv_optimized_model.pkl'}")
    print(f"   âœ… æŠ¥å‘Šä¿å­˜: {output_path / 'hv_training_report.json'}")
    
    # ä¸ç›®æ ‡å¯¹æ¯”
    print("\n" + "=" * 80)
    print("ğŸ¯ vs é«˜è´¨é‡ç›®æ ‡")
    print("=" * 80)
    
    target_r2 = 0.83
    target_mae = 125
    target_overfit = 1.08
    
    r2_status = "âœ…" if r2_cv >= target_r2 else "âš ï¸"
    mae_status = "âœ…" if mae_cv <= target_mae else "âš ï¸"
    overfit_status = "âœ…" if overfit_ratio <= target_overfit else "âš ï¸"
    
    print(f"   RÂ² (CV): {r2_cv:.4f} {r2_status} (ç›®æ ‡ â‰¥ {target_r2})")
    print(f"   MAE (CV): {mae_cv:.2f} HV {mae_status} (ç›®æ ‡ â‰¤ {target_mae})")
    print(f"   è¿‡æ‹Ÿåˆæ¯”ç‡: {overfit_ratio:.3f} {overfit_status} (ç›®æ ‡ â‰¤ {target_overfit})")
    
    # æ€»ä½“è¯„ä¼°
    all_pass = (r2_cv >= target_r2) and (mae_cv <= target_mae) and (overfit_ratio <= target_overfit)
    
    print("\n" + "=" * 80)
    if all_pass:
        print("ğŸ‰ æ­å–œï¼æ‰€æœ‰ç›®æ ‡å‡å·²è¾¾æˆï¼")
    else:
        print("ğŸ“Š æ¨¡å‹è´¨é‡è¯„ä¼°:")
        if r2_cv >= target_r2:
            print("   âœ… é¢„æµ‹ç²¾åº¦è¾¾æ ‡")
        else:
            print(f"   âš ï¸ ç²¾åº¦ç•¥ä½äºç›®æ ‡ (å·®è·: {target_r2 - r2_cv:.4f})")
        
        if mae_cv <= target_mae:
            print("   âœ… é¢„æµ‹è¯¯å·®è¾¾æ ‡")
        else:
            print(f"   âš ï¸ è¯¯å·®ç•¥é«˜äºç›®æ ‡ (å·®è·: {mae_cv - target_mae:.2f} HV)")
        
        if overfit_ratio <= target_overfit:
            print("   âœ… æ³›åŒ–èƒ½åŠ›è¾¾æ ‡")
        else:
            print(f"   âš ï¸ è¿‡æ‹Ÿåˆç•¥é«˜äºç›®æ ‡ (å·®è·: {overfit_ratio - target_overfit:.3f})")
    
    print("=" * 80)
    
    return report


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒOptunaä¼˜åŒ–çš„æœ€ç»ˆæ¨¡å‹')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data_fixed.csv')
    parser.add_argument('--features', type=str,
                       default='models/selected_features_top30.json')
    parser.add_argument('--params', type=str,
                       default='models/optuna_results/xgboost/best_params_xgboost.json')
    parser.add_argument('--output', type=str,
                       default='models/high_quality')
    
    args = parser.parse_args()
    
    train_final_optimized_model(
        data_path=args.data,
        features_path=args.features,
        params_path=args.params,
        output_dir=args.output
    )


if __name__ == "__main__":
    main()
