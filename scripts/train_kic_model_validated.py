"""
KICæ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆå¢å¼ºéªŒè¯ç‰ˆï¼‰

åŠŸèƒ½:
1. ä½¿ç”¨GBFSé€‰å®šçš„ç‰¹å¾è®­ç»ƒKICæ¨¡å‹
2. å®æ–½ä¸¥æ ¼çš„äº¤å‰éªŒè¯
3. è¿‡æ‹Ÿåˆæ£€æµ‹
4. ç”Ÿæˆè¯¦ç»†è®­ç»ƒæŠ¥å‘Š

ä½¿ç”¨:
    python scripts/train_kic_model_validated.py
    
ä½œè€…: HEACéªŒè¯æµç¨‹
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from xgboost import XGBRegressor
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error


def train_kic_model(data_path, features_path, output_dir='models/validated_kic'):
    """è®­ç»ƒKICé¢„æµ‹æ¨¡å‹"""
    
    print("=" * 80)
    print("ğŸš€ KICæ¨¡å‹è®­ç»ƒè„šæœ¬ï¼ˆå¢å¼ºéªŒè¯ç‰ˆï¼‰")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {data_path}")
    print(f"ğŸŒŸ ç‰¹å¾é…ç½®: {features_path}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    print("\n[1/6] åŠ è½½æ•°æ®...")
    df = pd.read_csv(data_path)
    print(f"âœ… åŠ è½½ {len(df)} æ¡è®°å½•")
    
    # åŠ è½½ç‰¹å¾åˆ—è¡¨
    print("\n[2/6] åŠ è½½ç‰¹å¾é…ç½®...")
    with open(features_path, 'r', encoding='utf-8') as f:
        feature_config = json.load(f)
    
    selected_features = feature_config['selected_features']
    print(f"âœ… ä½¿ç”¨ {len(selected_features)} ä¸ªé€‰å®šç‰¹å¾")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n[3/6] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    df_clean = df.dropna(subset=['kic'])
    print(f"   KICæœ‰æ•ˆè®°å½•: {len(df_clean)}")
    
    # æ£€æŸ¥ç‰¹å¾å¯ç”¨æ€§
    missing_features = [f for f in selected_features if f not in df_clean.columns]
    if missing_features:
        print(f"   âš ï¸  æ•°æ®ä¸­ç¼ºå¤± {len(missing_features)} ä¸ªç‰¹å¾:")
        for f in missing_features[:5]:
            print(f"      - {f}")
        selected_features = [f for f in selected_features if f in df_clean.columns]
        print(f"   è°ƒæ•´åä½¿ç”¨ {len(selected_features)} ä¸ªç‰¹å¾")
    
    X = df_clean[selected_features].copy()
    y = df_clean['kic'].copy()
    
    # å¡«å……ç¼ºå¤±å€¼
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    X = X.fillna(X.median())
    
    print(f"   âœ… è®­ç»ƒé›†: {len(X)} æ ·æœ¬, {len(selected_features)} ç‰¹å¾")
    print(f"   KICèŒƒå›´: [{y.min():.1f}, {y.max():.1f}], å‡å€¼: {y.mean():.1f}")
    
    # äº¤å‰éªŒè¯
    print("\n[4/6] äº¤å‰éªŒè¯è®­ç»ƒ...")
    
    # ä½¿ç”¨XGBoost
    model = XGBRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        n_jobs=-1,
        random_state=42
    )
    
    # 5-Foldäº¤å‰éªŒè¯
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    
    print("   æ‰§è¡Œ5-Foldäº¤å‰éªŒè¯...")
    y_pred_cv = cross_val_predict(model, X, y, cv=cv, n_jobs=1)
    
    # è®¡ç®—CVæŒ‡æ ‡
    r2_cv = r2_score(y, y_pred_cv)
    mae_cv = mean_absolute_error(y, y_pred_cv)
    rmse_cv = np.sqrt(mean_squared_error(y, y_pred_cv))
    
    print(f"\n   äº¤å‰éªŒè¯ç»“æœ:")
    print(f"      RÂ² Score: {r2_cv:.4f}")
    print(f"      MAE: {mae_cv:.4f} MPaÂ·m^1/2")
    print(f"      RMSE: {rmse_cv:.4f} MPaÂ·m^1/2")
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\n[5/6] è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    model.fit(X, y)
    
    # è®­ç»ƒé›†è¯„ä¼°ï¼ˆæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰
    y_pred_train = model.predict(X)
    r2_train = r2_score(y, y_pred_train)
    mae_train = mean_absolute_error(y, y_pred_train)
    rmse_train = np.sqrt(mean_squared_error(y, y_pred_train))
    
    print(f"\n   è®­ç»ƒé›†ç»“æœ:")
    print(f"      RÂ² Score: {r2_train:.4f}")
    print(f"      MAE: {mae_train:.4f} MPaÂ·m^1/2")
    print(f"      RMSE: {rmse_train:.4f} MPaÂ·m^1/2")
    
    # è¿‡æ‹Ÿåˆæ£€æµ‹
    overfitting_ratio = r2_train / r2_cv if r2_cv > 0 else float('inf')
    print(f"\n   è¿‡æ‹Ÿåˆæ£€æµ‹:")
    print(f"      RÂ² (Train/CV): {overfitting_ratio:.3f}")
    if overfitting_ratio > 1.1:
        print(f"      âš ï¸  å¯èƒ½å­˜åœ¨è½»åº¦è¿‡æ‹Ÿåˆ")
    elif overfitting_ratio > 1.2:
        print(f"      âš ï¸âš ï¸  å¯èƒ½å­˜åœ¨ä¸­åº¦è¿‡æ‹Ÿåˆ")
    else:
        print(f"      âœ… è¿‡æ‹Ÿåˆé£é™©ä½")
    
    # ç‰¹å¾é‡è¦æ€§
    print("\n   ç‰¹å¾é‡è¦æ€§ï¼ˆTop 15ï¼‰:")
    feature_importance = pd.DataFrame({
        'feature': selected_features,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for idx, row in feature_importance.head(15).iterrows():
        print(f"      {row['feature']:<40} {row['importance']:.4f}")
    
    # ä¿å­˜æ¨¡å‹å’Œç»“æœ
    print("\n[6/6] ä¿å­˜æ¨¡å‹å’Œç»“æœ...")
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    model_path = output_path / 'kic_validated_model.pkl'
    joblib.dump(model, model_path)
    print(f"   âœ… æ¨¡å‹: {model_path}")
    
    # ä¿å­˜ç‰¹å¾åˆ—è¡¨
    features_output_path = output_path / 'kic_feature_list.json'
    with open(features_output_path, 'w', encoding='utf-8') as f:
        json.dump(selected_features, f, indent=2, ensure_ascii=False)
    print(f"   âœ… ç‰¹å¾åˆ—è¡¨: {features_output_path}")
    
    # ä¿å­˜è®­ç»ƒæŠ¥å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'data_path': data_path,
        'sample_count': len(X),
        'feature_count': len(selected_features),
        'target': 'kic',
        'target_stats': {
            'mean': float(y.mean()),
            'std': float(y.std()),
            'min': float(y.min()),
            'max': float(y.max())
        },
        'cv_metrics': {
            'r2': float(r2_cv),
            'mae': float(mae_cv),
            'rmse': float(rmse_cv)
        },
        'train_metrics': {
            'r2': float(r2_train),
            'mae': float(mae_train),
            'rmse': float(rmse_train)
        },
        'overfitting_ratio': float(overfitting_ratio),
        'feature_importance': feature_importance.to_dict('records'),
        'model_params': model.get_params()
    }
    
    report_path = output_path / 'kic_training_report.json'
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print(f"   âœ… è®­ç»ƒæŠ¥å‘Š: {report_path}")
    
    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    
    return model, report


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒKICé¢„æµ‹æ¨¡å‹ï¼ˆå¢å¼ºéªŒè¯ç‰ˆï¼‰')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data_fixed.csv',
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--features', type=str,
                       default='models/selected_features_kic.json',
                       help='ç‰¹å¾é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str,
                       default='models/validated_kic',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    train_kic_model(
        data_path=args.data,
        features_path=args.features,
        output_dir=args.output
    )


if __name__ == "__main__":
    main()
