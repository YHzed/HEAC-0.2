"""
Materials Projectç¼“å­˜æµè§ˆå’Œç®¡ç†å·¥å…·

æ­¤è„šæœ¬ç”¨äºç®¡ç†å’ŒæŸ¥çœ‹MP APIçš„ç¼“å­˜æ•°æ®ã€‚

ç”¨æ³•:
    # æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
    python scripts/browse_mp_cache.py --stats
    
    # åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
    python scripts/browse_mp_cache.py --list
    
    # æŸ¥çœ‹ç‰¹å®šç¼“å­˜å†…å®¹
    python scripts/browse_mp_cache.py --view summary_TiO2.json
    
    # æ¸…ç†è¿‡æœŸç¼“å­˜
    python scripts/browse_mp_cache.py --clean
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.config import config


def get_cache_files():
    """è·å–æ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
    cache_dir = config.get_cache_path()
    if not cache_dir.exists():
        return []
    return list(cache_dir.glob('*.json'))


def format_size(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


def show_stats():
    """æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    cache_files = get_cache_files()
    
    if not cache_files:
        print("ğŸ“‚ ç¼“å­˜ç›®å½•ä¸ºç©º")
        return
    
    total_size = sum(f.stat().st_size for f in cache_files)
    
    # ç»Ÿè®¡è¿‡æœŸæ–‡ä»¶
    expired_count = 0
    ttl_days = config.MP_CACHE_TTL_DAYS
    cutoff_date = datetime.now() - timedelta(days=ttl_days)
    
    for cache_file in cache_files:
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cached_time = datetime.fromisoformat(data.get('_cached_at', '2000-01-01'))
                if cached_time < cutoff_date:
                    expired_count += 1
        except:
            pass
    
    print(f"\n{'='*60}")
    print(f"Materials Project ç¼“å­˜ç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"ç¼“å­˜ç›®å½•: {config.get_cache_path()}")
    print(f"ç¼“å­˜æ–‡ä»¶æ•°: {len(cache_files)}")
    print(f"æ€»å¤§å°: {format_size(total_size)}")
    print(f"ç¼“å­˜TTL: {ttl_days} å¤©")
    print(f"è¿‡æœŸæ–‡ä»¶æ•°: {expired_count}")
    print(f"{'='*60}\n")


def list_cache():
    """åˆ—å‡ºæ‰€æœ‰ç¼“å­˜æ–‡ä»¶"""
    cache_files = get_cache_files()
    
    if not cache_files:
        print("ğŸ“‚ ç¼“å­˜ç›®å½•ä¸ºç©º")
        return
    
    print(f"\n{'='*60}")
    print(f"ç¼“å­˜æ–‡ä»¶åˆ—è¡¨ (å…± {len(cache_files)} ä¸ª)")
    print(f"{'='*60}\n")
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    cache_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    
    for i, cache_file in enumerate(cache_files, 1):
        stat = cache_file.stat()
        mod_time = datetime.fromtimestamp(stat.st_mtime)
        size = format_size(stat.st_size)
        
        # è¯»å–ç¼“å­˜æ—¶é—´
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cached_at = data.get('_cached_at', 'N/A')
                if cached_at != 'N/A':
                    cached_time = datetime.fromisoformat(cached_at)
                    age = datetime.now() - cached_time
                    age_str = f"{age.days}å¤©å‰" if age.days > 0 else f"{age.seconds//3600}å°æ—¶å‰"
                else:
                    age_str = 'N/A'
        except:
            age_str = 'Error'
        
        print(f"{i:3d}. {cache_file.name}")
        print(f"     å¤§å°: {size:>10s} | ç¼“å­˜æ—¶é—´: {age_str}")
        
        if i % 10 == 0 and i < len(cache_files):
            print()


def view_cache(filename: str):
    """æŸ¥çœ‹ç‰¹å®šç¼“å­˜æ–‡ä»¶å†…å®¹"""
    cache_dir = config.get_cache_path()
    cache_file = cache_dir / filename
    
    if not cache_file.exists():
        print(f"âŒ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
        return
    
    try:
        with open(cache_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"\n{'='*60}")
        print(f"ç¼“å­˜æ–‡ä»¶: {filename}")
        print(f"{'='*60}")
        print(f"ç¼“å­˜æ—¶é—´: {data.get('_cached_at', 'N/A')}")
        print(f"\nå†…å®¹:")
        print(json.dumps(data.get('content'), ensure_ascii=False, indent=2))
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"âŒ è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")


def clean_cache(dry_run: bool = False):
    """æ¸…ç†è¿‡æœŸç¼“å­˜"""
    cache_files = get_cache_files()
    
    if not cache_files:
        print("ğŸ“‚ ç¼“å­˜ç›®å½•ä¸ºç©º")
        return
    
    ttl_days = config.MP_CACHE_TTL_DAYS
    cutoff_date = datetime.now() - timedelta(days=ttl_days)
    
    print(f"\n{'='*60}")
    print(f"æ¸…ç†è¿‡æœŸç¼“å­˜ (TTL: {ttl_days} å¤©)")
    if dry_run:
        print("(æ¨¡æ‹Ÿè¿è¡Œ - ä¸ä¼šå®é™…åˆ é™¤)")
    print(f"{'='*60}\n")
    
    deleted_count = 0
    deleted_size = 0
    
    for cache_file in cache_files:
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cached_time = datetime.fromisoformat(data.get('_cached_at', '2000-01-01'))
                
                if cached_time < cutoff_date:
                    size = cache_file.stat().st_size
                    age = datetime.now() - cached_time
                    
                    print(f"{'[æ¨¡æ‹Ÿ] ' if dry_run else ''}åˆ é™¤: {cache_file.name}")
                    print(f"  ç¼“å­˜æ—¶é—´: {cached_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    print(f"  å·²è¿‡æœŸ: {age.days} å¤©")
                    print(f"  å¤§å°: {format_size(size)}")
                    print()
                    
                    if not dry_run:
                        cache_file.unlink()
                    
                    deleted_count += 1
                    deleted_size += size
        except Exception as e:
            print(f"âŒ å¤„ç† {cache_file.name} æ—¶å‡ºé”™: {e}")
    
    print(f"{'='*60}")
    if deleted_count > 0:
        print(f"{'æ¨¡æ‹Ÿ' if dry_run else ''}åˆ é™¤äº† {deleted_count} ä¸ªæ–‡ä»¶")
        print(f"é‡Šæ”¾ç©ºé—´: {format_size(deleted_size)}")
    else:
        print("æ²¡æœ‰è¿‡æœŸçš„ç¼“å­˜æ–‡ä»¶")
    print(f"{'='*60}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Materials Projectç¼“å­˜ç®¡ç†å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
  python browse_mp_cache.py --stats
  
  # åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
  python browse_mp_cache.py --list
  
  # æŸ¥çœ‹ç‰¹å®šç¼“å­˜
  python browse_mp_cache.py --view summary_TiO2.json
  
  # æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆæ¨¡æ‹Ÿè¿è¡Œï¼‰
  python browse_mp_cache.py --clean --dry-run
  
  # æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆå®é™…åˆ é™¤ï¼‰
  python browse_mp_cache.py --clean
        """
    )
    
    parser.add_argument('--stats', action='store_true', 
                        help='æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--list', '-l', action='store_true', 
                        help='åˆ—å‡ºæ‰€æœ‰ç¼“å­˜æ–‡ä»¶')
    parser.add_argument('--view', '-v', type=str, metavar='FILENAME',
                        help='æŸ¥çœ‹ç‰¹å®šç¼“å­˜æ–‡ä»¶å†…å®¹')
    parser.add_argument('--clean', '-c', action='store_true', 
                        help='æ¸…ç†è¿‡æœŸç¼“å­˜')
    parser.add_argument('--dry-run', action='store_true', 
                        help='æ¨¡æ‹Ÿè¿è¡Œï¼ˆä¸å®é™…åˆ é™¤æ–‡ä»¶ï¼‰')
    
    args = parser.parse_args()
    
    # æ ¹æ®å‚æ•°æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.stats:
        show_stats()
    elif args.list:
        list_cache()
    elif args.view:
        view_cache(args.view)
    elif args.clean:
        clean_cache(dry_run=args.dry_run)
    else:
        # é»˜è®¤æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        show_stats()
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
