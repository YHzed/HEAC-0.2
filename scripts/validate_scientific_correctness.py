"""
ç§‘å­¦æ€§éªŒè¯è„šæœ¬

åŠŸèƒ½:
1. ç‰©ç†ç‰¹å¾è®¡ç®—éªŒè¯
2. HEAåˆ¤å®šé€»è¾‘éªŒè¯
3. ç‰¹å¾-æ€§èƒ½ç›¸å…³æ€§éªŒè¯
4. é¢„æµ‹åˆç†æ€§éªŒè¯

ä½¿ç”¨:
    python scripts/validate_scientific_correctness.py
    
ä½œè€…: HEACéªŒè¯æµç¨‹
æ—¥æœŸ: 2026-01-15
"""

import sys
import os
import argparse
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def validate_physics_calculations(df):
    """éªŒè¯ç‰©ç†ç‰¹å¾è®¡ç®—"""
    
    print("\n[1/4] ç‰©ç†ç‰¹å¾è®¡ç®—éªŒè¯")
    print("=" * 80)
    
    issues = []
    
    # æ£€æŸ¥lattice mismatchè®¡ç®—
    if 'lattice_mismatch' in df.columns and 'pred_lattice_param' in df.columns:
        lm = df['lattice_mismatch'].dropna()
        print(f"\n   Lattice Mismatch:")
        print(f"      èŒƒå›´: [{lm.min():.4f}, {lm.max():.4f}]")
        print(f"      å‡å€¼: {lm.mean():.4f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼ï¼ˆlattice mismatchåº”è¯¥æ˜¯å°çš„æ­£æ•°ï¼‰
        if lm.min() < 0:
            print(f"      âš ï¸  å‘ç°è´Ÿå€¼ï¼Œå¯èƒ½è®¡ç®—æœ‰è¯¯")
            issues.append({'feature': 'lattice_mismatch', 'issue': 'negative_values'})
        elif lm.max() > 1:
            print(f"      âš ï¸  å‘ç°è¿‡å¤§å€¼ï¼ˆ> 1ï¼‰ï¼Œå¯èƒ½ä¸åˆç†")
            issues.append({'feature': 'lattice_mismatch', 'issue': 'extreme_values'})
        else:
            print(f"      âœ… æ•°å€¼èŒƒå›´åˆç†")
    
    # æ£€æŸ¥VEC ï¼ˆValence Electron Concentrationï¼‰
    if 'vec_binder' in df.columns:
        vec = df['vec_binder'].dropna()
        print(f"\n   VEC (Binder):")
        print(f"      èŒƒå›´: [{vec.min():.2f}, {vec.max():.2f}]")
        print(f"      å‡å€¼: {vec.mean():.2f}")
        
        # VECé€šå¸¸åœ¨4-11èŒƒå›´å†…
        if vec.min() < 3 or vec.max() > 12:
            print(f"      âš ï¸  VECå€¼è¶…å‡ºå¸¸è§èŒƒå›´ï¼ˆ3-12ï¼‰")
            issues.append({'feature': 'vec_binder', 'issue': 'out_of_range'})
        else:
            print(f"      âœ… VECèŒƒå›´åˆç†")
    
    # æ£€æŸ¥å¯†åº¦
    if 'binder_density' in df.columns:
        density = df['binder_density'].dropna()
        print(f"\n   Binder Density:")
        print(f"      èŒƒå›´: [{density.min():.2f}, {density.max():.2f}] g/cmÂ³")
        print(f"      å‡å€¼: {density.mean():.2f} g/cmÂ³")
        
        # é‡‘å±å¯†åº¦é€šå¸¸åœ¨2-20 g/cmÂ³
        if density.min() < 1 or density.max() > 25:
            print(f"      âš ï¸  å¯†åº¦å€¼ä¸åˆç†")
            issues.append({'feature': 'binder_density', 'issue': 'unrealistic_values'})
        else:
            print(f"      âœ… å¯†åº¦èŒƒå›´åˆç†")
    
    # æ£€æŸ¥formation energy
    if 'pred_formation_energy' in df.columns:
        fe = df['pred_formation_energy'].dropna()
        print(f"\n   Formation Energy:")
        print(f"      èŒƒå›´: [{fe.min():.4f}, {fe.max():.4f}] eV/atom")
        print(f"      å‡å€¼: {fe.mean():.4f} eV/atom")
        
        # Formation energyé€šå¸¸æ˜¯è´Ÿå€¼ï¼ˆç¨³å®šæ€ï¼‰
        positive_ratio = (fe > 0).sum() / len(fe) * 100
        if positive_ratio > 10:
            print(f"      âš ï¸  {positive_ratio:.1f}% çš„å€¼ä¸ºæ­£ï¼ˆä¸ç¨³å®šï¼‰ï¼Œæ¯”ä¾‹åé«˜")
            issues.append({'feature': 'pred_formation_energy', 'issue': 'high_positive_ratio'})
        else:
            print(f"      âœ… Formation energyåˆ†å¸ƒåˆç†")
    
    return issues


def validate_hea_classification(df):
    """éªŒè¯HEAåˆ¤å®šé€»è¾‘"""
    
    print("\n[2/4] HEAåˆ¤å®šé€»è¾‘éªŒè¯")
    print("=" * 80)
    
    issues = []
    
    if 'is_hea' in df.columns and 'element_count' in df.columns:
        # HEAå®šä¹‰ï¼šé€šå¸¸éœ€è¦ >= 5ä¸ªå…ƒç´ ï¼Œä¸”æ‘©å°”åˆ†æ•°éƒ½åœ¨5-35%ä¹‹é—´
        hea_count = df['is_hea'].sum()
        total_count = len(df)
        
        print(f"   HEAæ ·æœ¬: {hea_count} / {total_count} ({hea_count/total_count*100:.1f}%)")
        
        # åˆ†æå…ƒç´ æ•°é‡åˆ†å¸ƒ
        print(f"\n   å…ƒç´ æ•°é‡åˆ†å¸ƒ:")
        for elem_count in sorted(df['element_count'].dropna().unique()):
            count = (df['element_count'] == elem_count).sum()
            hea_in_this = ((df['element_count'] == elem_count) & (df['is_hea'] == True)).sum()
            print(f"      {int(elem_count)}å…ƒ: {count} æ ·æœ¬, HEA: {hea_in_this}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰<5å…ƒç´ è¢«æ ‡è®°ä¸ºHEA
        false_hea = df[(df['element_count'] < 5) & (df['is_hea'] == True)]
        if len(false_hea) > 0:
            print(f"\n      âš ï¸  å‘ç° {len(false_hea)} ä¸ª<5å…ƒç´ ä½†è¢«æ ‡è®°ä¸ºHEAçš„æ ·æœ¬")
            issues.append({
                'issue': 'low_element_marked_as_hea',
                'count': len(false_hea),
                'severity': 'error'
            })
        else:
            print(f"\n      âœ… æœªå‘ç°ä½å…ƒç´ æ•°è¯¯åˆ¤ä¸ºHEA")
        
        # æ£€æŸ¥äºŒå…ƒåˆé‡‘ï¼ˆåº”è¯¥ä¸æ˜¯HEAï¼‰
        binary_alloys = df[df['element_count'] == 2]
        if len(binary_alloys) > 0:
            binary_hea = binary_alloys[binary_alloys['is_hea'] == True]
            if len(binary_hea) > 0:
                print(f"      âŒ ä¸¥é‡é”™è¯¯: {len(binary_hea)} ä¸ªäºŒå…ƒåˆé‡‘è¢«è¯¯åˆ¤ä¸ºHEA!")
                issues.append({
                    'issue': 'binary_alloy_as_hea',
                    'count': len(binary_hea),
                    'severity': 'critical'
                })
            else:
                print(f"      âœ… äºŒå…ƒåˆé‡‘æœªè¢«è¯¯åˆ¤ä¸ºHEA")
    else:
        print(f"   âš ï¸  ç¼ºå°‘HEAåˆ†ç±»å­—æ®µï¼Œæ— æ³•éªŒè¯")
    
    return issues


def validate_feature_correlation(df, target='hv'):
    """éªŒè¯ç‰¹å¾-æ€§èƒ½ç›¸å…³æ€§çš„ç‰©ç†æ„ä¹‰"""
    
    print("\n[3/4] ç‰¹å¾-æ€§èƒ½ç›¸å…³æ€§éªŒè¯")
    print("=" * 80)
    
    # å®šä¹‰é¢„æœŸçš„ç›¸å…³æ€§
    expected_correlations = {
        'binder_vol_pct': {
            'hv': 'negative',  # ç²˜ç»“ç›¸å¢åŠ ï¼Œç¡¬åº¦é™ä½
            'kic': 'positive'   # ç²˜ç»“ç›¸å¢åŠ ï¼ŒéŸ§æ€§æé«˜
        },
        'grain_size_um': {
            'hv': 'negative',  # Hall-Petch: æ™¶ç²’ç»†åŒ–æé«˜ç¡¬åº¦
            'kic': 'positive'   # æ™¶ç²’å¢å¤§æé«˜éŸ§æ€§
        },
        'sinter_temp_c': {
            'hv': 'positive',  # çƒ§ç»“æ¸©åº¦æé«˜ï¼Œè‡´å¯†åº¦æé«˜ï¼Œç¡¬åº¦æé«˜
            'kic': 'uncertain'
        }
    }
    
    issues = []
    
    target_data = df[target].dropna()
    
    print(f"   åˆ†æä¸ {target.upper()} çš„ç›¸å…³æ€§:\n")
    
    for feature, expectations in expected_correlations.items():
        if feature in df.columns and target in expectations:
            valid_idx = df[feature].notna() & target_data.notna()
            if valid_idx.sum() > 10:
                corr = np.corrcoef(df.loc[valid_idx, feature], target_data.loc[valid_idx])[0, 1]
                expected = expectations[target]
                
                # æ£€æŸ¥ç›¸å…³æ€§æ˜¯å¦ç¬¦åˆé¢„æœŸ
                ç¬¦åˆé¢„æœŸ = False
                if expected == 'positive' and corr > 0.1:
                    ç¬¦åˆé¢„æœŸ = True
                    status = "âœ…"
                elif expected == 'negative' and corr < -0.1:
                    ç¬¦åˆé¢„æœŸ = True
                    status = "âœ…"
                elif expected == 'uncertain':
                    ç¬¦åˆé¢„æœŸ = True
                    status = "â„¹ï¸"
                else:
                    status = "âš ï¸"
                    issues.append({
                        'feature': feature,
                        'target': target,
                        'expected': expected,
                        'actual_corr': float(corr),
                        'severity': 'warning'
                    })
                
                print(f"      {status} {feature:<30} r = {corr:>7.4f}  (é¢„æœŸ: {expected})")
    
    return issues


def validate_prediction_range(model_path, data_path, feature_list_path):
    """éªŒè¯é¢„æµ‹å€¼åˆç†æ€§"""
    
    print("\n[4/4] é¢„æµ‹åˆç†æ€§éªŒè¯")
    print("=" * 80)
    
    import joblib
    
    if not os.path.exists(model_path):
        print(f"   âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return {}
    
    # åŠ è½½æ¨¡å‹å’Œç‰¹å¾
    model = joblib.load(model_path)
    
    with open(feature_list_path, 'r') as f:
        features = json.load(f)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(data_path)
    df_clean = df.dropna(subset=['hv'])
    
    # å‡†å¤‡æ•°æ®
    missing_features = [f for f in features if f not in df_clean.columns]
    if missing_features:
        print(f"   âš ï¸  ç¼ºå°‘ {len(missing_features)} ä¸ªç‰¹å¾ï¼Œè·³è¿‡éªŒè¯")
        return {}
    
    X = df_clean[features].fillna(df_clean[features].median())
    y_true = df_clean['hv']
    
    # é¢„æµ‹
    y_pred = model.predict(X)
    
    print(f"   é¢„æµ‹å€¼åˆ†æ:")
    print(f"      çœŸå®å€¼èŒƒå›´: [{y_true.min():.1f}, {y_true.max():.1f}]")
    print(f"      é¢„æµ‹å€¼èŒƒå›´: [{y_pred.min():.1f}, {y_pred.max():.1f}]")
    
    # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦åˆç†
    issues = []
    
    # 1. æ˜¯å¦æœ‰è´Ÿå€¼
    negative_pred = (y_pred < 0).sum()
    if negative_pred > 0:
        print(f"      âŒ å‘ç° {negative_pred} ä¸ªè´Ÿå€¼é¢„æµ‹ï¼ˆç¡¬åº¦ä¸åº”ä¸ºè´Ÿï¼‰")
        issues.append({'issue': 'negative_predictions', 'count': int(negative_pred)})
    else:
        print(f"      âœ… æ— è´Ÿå€¼é¢„æµ‹")
    
    # 2. æ˜¯å¦æœ‰è¶…å‡ºåˆç†èŒƒå›´çš„å€¼
    # WC-Coç¡¬åº¦é€šå¸¸åœ¨1000-2500 HVèŒƒå›´
    unrealistic_low = (y_pred < 500).sum()
    unrealistic_high = (y_pred > 3000).sum()
    
    if unrealistic_low > 0:
        print(f"      âš ï¸  {unrealistic_low} ä¸ªé¢„æµ‹å€¼ < 500 HVï¼ˆå¯èƒ½è¿‡ä½ï¼‰")
        issues.append({'issue': 'unrealistic_low', 'count': int(unrealistic_low)})
    
    if unrealistic_high > 0:
        print(f"      âš ï¸  {unrealistic_high} ä¸ªé¢„æµ‹å€¼ > 3000 HVï¼ˆå¯èƒ½è¿‡é«˜ï¼‰")
        issues.append({'issue': 'unrealistic_high', 'count': int(unrealistic_high)})
    
    if unrealistic_low == 0 and unrealistic_high == 0:
        print(f"      âœ… é¢„æµ‹å€¼èŒƒå›´åˆç†")
    
    return {
        'pred_min': float(y_pred.min()),
        'pred_max': float(y_pred.max()),
        'pred_mean': float(y_pred.mean()),
        'issues': issues
    }


def main():
    parser = argparse.ArgumentParser(description='éªŒè¯ç§‘å­¦æ€§é—®é¢˜')
    parser.add_argument('--data', type=str,
                       default='datasets/exported_training_data.csv',
                       help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--model', type=str,
                       default='models/validated/hv_validated_model.pkl',
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--features', type=str,
                       default='models/validated/hv_feature_list.json',
                       help='ç‰¹å¾åˆ—è¡¨è·¯å¾„')
    parser.add_argument('--output', type=str,
                       default='models/scientific_validation_report.json',
                       help='è¾“å‡ºæŠ¥å‘Šè·¯å¾„')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ”¬ ç§‘å­¦æ€§éªŒè¯è„šæœ¬")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®: {args.data}")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(args.data)
    print(f"\nâœ… åŠ è½½ {len(df)} æ¡è®°å½•")
    
    # æ‰§è¡Œå„é¡¹éªŒè¯
    report = {
        'timestamp': datetime.now().isoformat(),
        'data_path': args.data
    }
    
    report['physics_calculations'] = validate_physics_calculations(df)
    report['hea_classification'] = validate_hea_classification(df)
    report['feature_correlation'] = validate_feature_correlation(df, target='hv')
    
    # å¦‚æœæ¨¡å‹å­˜åœ¨ï¼ŒéªŒè¯é¢„æµ‹
    if os.path.exists(args.model) and os.path.exists(args.features):
        report['prediction_validation'] = validate_prediction_range(
            args.model, args.data, args.features
        )
    else:
        print(f"\n   âš ï¸  æ¨¡å‹æˆ–ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡é¢„æµ‹éªŒè¯")
        report['prediction_validation'] = {}
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("ğŸ“Š ç§‘å­¦æ€§éªŒè¯æ‘˜è¦")
    print("=" * 80)
    
    total_issues = 0
    critical_issues = 0
    
    for key, issues in report.items():
        if isinstance(issues, list):
            total_issues += len(issues)
            critical_issues += sum(1 for i in issues if isinstance(i, dict) and i.get('severity') == 'critical')
    
    print(f"å‘ç° {total_issues} ä¸ªé—®é¢˜")
    if critical_issues > 0:
        print(f"âŒ å…¶ä¸­ {critical_issues} ä¸ªä¸¥é‡é—®é¢˜éœ€è¦ç«‹å³ä¿®å¤")
    elif total_issues > 0:
        print(f"âš ï¸  å»ºè®®å…³æ³¨å¹¶æ”¹è¿›è¿™äº›é—®é¢˜")
    else:
        print(f"âœ… ç§‘å­¦æ€§æ£€æŸ¥é€šè¿‡ï¼")
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {output_path}")
    print("=" * 80)


if __name__ == "__main__":
    main()
