"""
æ•°æ®åº“ç®¡ç†ç³»ç»Ÿé›†æˆæµ‹è¯•

éªŒè¯ï¼š
1. æ•°æ®åº“åŸºæœ¬æ“ä½œ
2. æŸ¥è¯¢åŠŸèƒ½
3. ä¸ ML Pipeline çš„é›†æˆ
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
from core.db_manager import CermetDB
from core.db_config import STANDARD_SCHEMA

def test_database_operations():
    """æµ‹è¯•æ•°æ®åº“åŸºæœ¬æ“ä½œ"""
    print("="*80)
    print("æµ‹è¯• 1: æ•°æ®åº“åŸºæœ¬æ“ä½œ")
    print("="*80)
    
    # è¿æ¥æ•°æ®åº“
    db = CermetDB('cermet_materials.db')
    
    # æµ‹è¯•ç»Ÿè®¡åŠŸèƒ½
    stats = db.get_statistics()
    print(f"\nâœ… æ•°æ®åº“ç»Ÿè®¡:")
    print(f"   æ€»è®°å½•æ•°: {stats['total_records']}")
    print(f"   HEA è®°å½•: {stats['hea_records']}")
    print(f"   ä¼ ç»Ÿè®°å½•: {stats['traditional_records']}")
    
    assert stats['total_records'] > 0, "æ•°æ®åº“åº”è¯¥åŒ…å«æ•°æ®"
    print("\nâœ… æµ‹è¯•é€šè¿‡: æ•°æ®åº“åŒ…å«æ•°æ®")
    
    return stats

def test_query_filters():
    """æµ‹è¯•æŸ¥è¯¢ç­›é€‰åŠŸèƒ½"""
    print("\n" + "="*80)
    print("æµ‹è¯• 2: æŸ¥è¯¢ç­›é€‰åŠŸèƒ½")
    print("="*80)
    
    db = CermetDB('cermet_materials.db')
    
    # åœºæ™¯ A: ä»…æŸ¥è¯¢ HEA æ•°æ®
    print("\nåœºæ™¯ A: ä»…æŸ¥è¯¢ HEA ç²˜ç»“ç›¸")
    df_hea = db.fetch_data(filters={'is_hea': 1})
    print(f"   æ‰¾åˆ° {len(df_hea)} æ¡ HEA æ•°æ®")
    assert all(df_hea['is_hea'] == 1), "æ‰€æœ‰è®°å½•åº”è¯¥æ˜¯ HEA"
    print("   âœ… ç­›é€‰æ­£ç¡®")
    
    # åœºæ™¯ B: å¿…é¡»åŒ…å« HV æ•°æ®
    print("\nåœºæ™¯ B: å¿…é¡»åŒ…å« HV æ•°æ®")
    df_hv = db.fetch_data(drop_na_cols=['hv'])
    null_count = df_hv['hv'].isnull().sum()
    print(f"   æ‰¾åˆ° {len(df_hv)} æ¡æ•°æ®ï¼ŒHV ç¼ºå¤±: {null_count}")
    assert null_count == 0, "ä¸åº”è¯¥æœ‰ HV ç¼ºå¤±å€¼"
    print("   âœ… ç¼ºå¤±å€¼ç­›é€‰æ­£ç¡®")
    
    # åœºæ™¯ C: æ¸©åº¦èŒƒå›´ç­›é€‰
    print("\nåœºæ™¯ C: çƒ§ç»“æ¸©åº¦ 1200-1600Â°C")
    df_temp = db.fetch_data(filters={'sinter_temp_c': (1200, 1600)})
    print(f"   æ‰¾åˆ° {len(df_temp)} æ¡æ•°æ®")
    if len(df_temp) > 0:
        df_temp_valid = df_temp.dropna(subset=['sinter_temp_c'])
        if len(df_temp_valid) > 0:
            assert all(df_temp_valid['sinter_temp_c'] >= 1200), "æ¸©åº¦åº” >= 1200"
            assert all(df_temp_valid['sinter_temp_c'] <= 1600), "æ¸©åº¦åº” <= 1600"
            print("   âœ… æ¸©åº¦èŒƒå›´ç­›é€‰æ­£ç¡®")
    
    print("\nâœ… æ‰€æœ‰æŸ¥è¯¢æµ‹è¯•é€šè¿‡")

def test_ml_pipeline_integration():
    """æµ‹è¯•ä¸ ML Pipeline çš„é›†æˆ"""
    print("\n" + "="*80)
    print("æµ‹è¯• 3: ML Pipeline é›†æˆ")
    print("="*80)
    
    db = CermetDB('cermet_materials.db')
    
    # æå–è®­ç»ƒæ•°æ®
    print("\næå–è®­ç»ƒæ•°æ® (HEA + å®Œæ•´å·¥è‰ºå‚æ•°)")
    df_train = db.fetch_data(
        filters={'is_hea': 1},
        drop_na_cols=['hv', 'grain_size_um', 'sinter_temp_c']
    )
    
    print(f"   æ‰¾åˆ° {len(df_train)} æ¡å¯ç”¨äºè®­ç»ƒçš„æ•°æ®")
    print(f"   åˆ—å: {list(df_train.columns[:10])}...")
    
    # éªŒè¯å…³é”®åˆ—å­˜åœ¨
    required_cols = ['composition_raw', 'hv', 'grain_size_um', 'sinter_temp_c']
    for col in required_cols:
        assert col in df_train.columns, f"ç¼ºå°‘å¿…è¦åˆ—: {col}"
    
    print(f"\n   âœ… æ•°æ®æ ¼å¼æ­£ç¡®")
    
    # æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
    if len(df_train) > 0:
        print("\n   æ•°æ®ç¤ºä¾‹:")
        sample = df_train[['composition_raw', 'hv', 'grain_size_um', 'sinter_temp_c']].head(3)
        for idx, row in sample.iterrows():
            print(f"     {idx}: {row['composition_raw'][:30]} | HV={row['hv']:.0f}")
    
    print("\nâœ… ML Pipeline é›†æˆæµ‹è¯•é€šè¿‡")
    
    return df_train

def test_field_mapping():
    """æµ‹è¯•å­—æ®µæ˜ å°„åŠŸèƒ½"""
    print("\n" + "="*80)
    print("æµ‹è¯• 4: å­—æ®µæ˜ å°„")
    print("="*80)
    
    # æµ‹è¯•åˆ—åè¯†åˆ«
    test_columns = [
        'HV, kgf/mm2',
        'KIC, MPaÂ·m1/2',
        'Grain_Size_um',
        'd, mm',
        'Composition'
    ]
    
    print("\næµ‹è¯•åˆ—åæ˜ å°„:")
    from core.db_config import get_standard_field_name
    
    for col in test_columns:
        std_field = get_standard_field_name(col)
        status = "âœ…" if std_field else "âŒ"
        print(f"   {status} '{col}' -> {std_field}")
    
    print("\nâœ… å­—æ®µæ˜ å°„æµ‹è¯•å®Œæˆ")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("ğŸ§ª é‡‘å±é™¶ç“·æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ - é›†æˆæµ‹è¯•")
    print("\n")
    
    try:
        # è¿è¡Œæµ‹è¯•
        stats = test_database_operations()
        test_query_filters()
        df_train = test_ml_pipeline_integration()
        test_field_mapping()
        
        # æ€»ç»“
        print("\n" + "="*80)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("="*80)
        print(f"\næ•°æ®åº“çŠ¶æ€:")
        print(f"  â€¢ æ€»æ•°æ®é‡: {stats['total_records']} æ¡")
        print(f"  â€¢ å¯ç”¨äºè®­ç»ƒçš„ HEA æ•°æ®: {len(df_train)} æ¡")
        print(f"  â€¢ HV å®Œæ•´æ€§: {stats['field_completeness']['hv']['completeness_pct']:.1f}%")
        print(f"  â€¢ KIC å®Œæ•´æ€§: {stats['field_completeness']['kic']['completeness_pct']:.1f}%")
        
        print("\nğŸ‰ æ•°æ®åº“ç®¡ç†ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
        print("\nå»ºè®®ä¸‹ä¸€æ­¥:")
        print("  1. è¿è¡Œ Streamlit åº”ç”¨: streamlit run app.py")
        print("  2. è®¿é—® 'æ•°æ®åº“ç®¡ç†' é¡µé¢")
        print("  3. å°è¯•å•æ¡å½•å…¥å’Œæ‰¹é‡å¯¼å…¥åŠŸèƒ½")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main())
