import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
import warnings

warnings.filterwarnings('ignore')

# ========== æ€§èƒ½ä¼˜åŒ–: æ•°æ®åŠ è½½ç¼“å­˜ ==========
@st.cache_data
def load_training_data(file_path: str) -> pd.DataFrame:
    """ç¼“å­˜è®­ç»ƒæ•°æ®åŠ è½½ - é¿å…é‡å¤è¯»å–CSV"""
    return pd.read_csv(file_path)

st.set_page_config(page_title="Model Training", page_icon="ğŸ“", layout="wide")

import ui.style_manager as style_manager
style_manager.apply_theme()

style_manager.ui_header("ğŸ“ HEA Model Training & Analysis")
st.markdown("""
**æ ¸å¿ƒæµç¨‹**ï¼š
1. æ•°æ®åŠ è½½ä¸åˆ†å‰²
2. K-Foldäº¤å‰éªŒè¯ï¼ˆè¯„ä¼°çœŸå®æ³›åŒ–èƒ½åŠ›ï¼‰
3. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰
4. æ¨¡å‹è®­ç»ƒ
5. **SHAPç‰©ç†å¯è§£é‡Šæ€§åˆ†æ**ï¼ˆå…³é”®æ­¥éª¤ï¼‰
6. æ¨¡å‹ä¿å­˜
""")

# ===================
# Step 1: æ•°æ®åŠ è½½
# ===================
st.header("ğŸ“ Step 1: Load Selected Features")

file_path = st.text_input(
    "ç²¾ç®€ç‰¹å¾é›†è·¯å¾„",
    value=r"d:\ML\HEAC 0.2\datasets\hea_selected_features.csv"
)

if st.button("Load Data") or 'df_model' in st.session_state:
    try:
        if 'df_model' not in st.session_state:
            # ä½¿ç”¨ç¼“å­˜åŠ è½½æ•°æ®
            df = load_training_data(file_path)
            st.session_state.df_model = df
        else:
            df = st.session_state.df_model
        
        st.success(f"âœ“ åŠ è½½äº† {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        
        # æ™ºèƒ½è¯†åˆ«ç›®æ ‡å˜é‡ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        # å®šä¹‰ç›®æ ‡å˜é‡çš„å…³é”®è¯åŠå…¶å˜ä½“
        target_mappings = {
            'Hardness': ['HV, kgf/mm2', 'HV_kgf_mm2', 'HV', 'Hardness'],
            'Strength': ['TRS, MPa', 'TRS_MPa', 'TRS', 'Strength'],
            'Toughness': ['KIC, MPaÂ·m1/2', 'KIC_MPa_m', 'KIC', 'Toughness']
        }
        
        available_targets = []
        target_info = []
        
        for category, variants in target_mappings.items():
            for variant in variants:
                if variant in df.columns:
                    # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
                    missing_pct = df[variant].isna().sum() / len(df) * 100
                    valid_count = df[variant].notna().sum()
                    
                    # åªä¿ç•™ç¼ºå¤±ç‡<50%ä¸”æœ‰æ•ˆå€¼>10çš„ç›®æ ‡å˜é‡
                    if missing_pct < 50 and valid_count >= 10:
                        available_targets.append(variant)
                        target_info.append(f"{variant} (æœ‰æ•ˆå€¼: {valid_count}, ç¼ºå¤±: {missing_pct:.1f}%)")
                        break  # æ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„å°±è·³å‡º
                    else:
                        st.warning(f"âš ï¸ è·³è¿‡ `{variant}`: ç¼ºå¤±ç‡ {missing_pct:.1f}% è¿‡é«˜æˆ–æœ‰æ•ˆå€¼ä¸è¶³")
        
        if available_targets:
            st.session_state.available_targets = available_targets
            st.success(f"âœ“ æ‰¾åˆ° {len(available_targets)} ä¸ªå¯ç”¨ç›®æ ‡å˜é‡")
            with st.expander("ğŸ“Š ç›®æ ‡å˜é‡è¯¦æƒ…"):
                for info in target_info:
                    st.write(f"- {info}")
        else:
            st.error("âš ï¸ æ— å¯ç”¨ç›®æ ‡å˜é‡ï¼è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
            st.info("æç¤ºï¼šç³»ç»Ÿä¼šè‡ªåŠ¨è·³è¿‡ç¼ºå¤±ç‡>50%æˆ–æœ‰æ•ˆå€¼<10çš„ç›®æ ‡å˜é‡ã€‚")
        
        with st.expander("ğŸ“Š æ•°æ®é¢„è§ˆ"):
            st.dataframe(df.head())
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ€»æ ·æœ¬æ•°", df.shape[0])
            with col2:
                st.metric("ç‰¹å¾æ•°", df.shape[1] - len(available_targets))
            with col3:
                st.metric("ç›®æ ‡å˜é‡æ•°", len(available_targets))
    
    except Exception as e:
        st.error(f"åŠ è½½å¤±è´¥: {e}")

# ===================
# Step 2: æ•°æ®å‡†å¤‡
# ===================
if 'df_model' in st.session_state:
    st.header("ğŸ”§ Step 2: Data Preparation")
    
    df = st.session_state.df_model
    
    col1, col2 = st.columns(2)
    
    with col1:
        selected_target = st.selectbox(
            "ğŸ¯ é€‰æ‹©ç›®æ ‡å˜é‡",
            st.session_state.available_targets,
            help="é€‰æ‹©è¦é¢„æµ‹çš„æ€§èƒ½æŒ‡æ ‡"
        )
    
    with col2:
        normalize = st.checkbox(
            "æ ‡å‡†åŒ–ç‰¹å¾ï¼ˆStandardScalerï¼‰",
            value=True,
            help="æ¨èå¼€å¯ï¼Œæé«˜æ¨¡å‹ç¨³å®šæ€§"
        )
    
    # æ•°æ®åˆ†å‰²å‚æ•°
    st.subheader("æ•°æ®åˆ†å‰²è®¾ç½®")
    col1, col2 = st.columns(2)
    
    with col1:
        test_size = st.slider(
            "æµ‹è¯•é›†æ¯”ä¾‹",
            min_value=0.1,
            max_value=0.3,
            value=0.2,
            step=0.05
        )
    
    with col2:
        random_state = st.number_input(
            "éšæœºç§å­",
            value=42,
            help="ä¿è¯ç»“æœå¯å¤ç°"
        )
    
    if st.button("å‡†å¤‡æ•°æ®"):
        with st.spinner("å‡†å¤‡æ•°æ®ä¸­..."):
            # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
            feature_cols = [c for c in df.columns if c not in st.session_state.available_targets]
            
            X = df[feature_cols].copy()
            y = df[selected_target].copy()
            
            # å¤„ç†ç¼ºå¤±å€¼
            y = pd.to_numeric(y, errors='coerce')
            valid_idx = y.notna()
            
            X = X[valid_idx]
            y = y[valid_idx]
            
            st.info(f"ç§»é™¤äº† {(~valid_idx).sum()} è¡Œç¼ºå¤±ç›®æ ‡å€¼")
            
            # Train/Teståˆ†å‰²
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=test_size,
                random_state=random_state
            )
            
            # æ ‡å‡†åŒ–
            if normalize:
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                # è½¬å›DataFrameä¿ç•™åˆ—å
                X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
                X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
                
                st.session_state.scaler = scaler
            else:
                st.session_state.scaler = None
            
            # ä¿å­˜åˆ°session_state
            st.session_state.X_train = X_train
            st.session_state.X_test = X_test
            st.session_state.y_train = y_train
            st.session_state.y_test = y_test
            st.session_state.selected_target = selected_target
            st.session_state.feature_names = feature_cols
            
            st.success("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")
            
            # æ˜¾ç¤ºä¿¡æ¯
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("è®­ç»ƒé›†", f"{len(X_train)} æ ·æœ¬")
            with col2:
                st.metric("æµ‹è¯•é›†", f"{len(X_test)} æ ·æœ¬")
            with col3:
                st.metric("ç‰¹å¾æ•°", len(feature_cols))

# ===================
# Step 3: K-Foldäº¤å‰éªŒè¯ + è¶…å‚æ•°ä¼˜åŒ–
# ===================
if 'X_train' in st.session_state:
    st.header("âš™ï¸ Step 3: Hyperparameter Optimization with K-Fold CV")
    
    st.markdown("""
    > âš ï¸ **é‡è¦**ï¼šå› ä¸ºæ•°æ®é‡æœ‰é™ï¼Œæˆ‘ä»¬ä½¿ç”¨**K-Foldäº¤å‰éªŒè¯**æ¥è¯„ä¼°æ¨¡å‹çš„çœŸå®æ³›åŒ–èƒ½åŠ›ã€‚
    > ä¸ç›´æ¥fitå®Œå°±ç»“æŸï¼Œè€Œæ˜¯é€šè¿‡å¤šæŠ˜éªŒè¯ç¡®ä¿æ¨¡å‹ç¨³å¥æ€§ã€‚
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        cv_folds = st.slider(
            "K-FoldæŠ˜æ•°",
            min_value=3,
            max_value=10,
            value=5,
            help="æ•°æ®é‡è¾ƒå°æ—¶å»ºè®®5-10æŠ˜"
        )
    
    with col2:
        n_trials = st.number_input(
            "Optunaä¼˜åŒ–æ¬¡æ•°",
            min_value=10,
            max_value=200,
            value=50,
            help="è¯•éªŒæ¬¡æ•°è¶Šå¤šï¼Œæ‰¾åˆ°æœ€ä¼˜å‚æ•°çš„å¯èƒ½æ€§è¶Šå¤§"
        )
    
    if st.button("ğŸš€ å¼€å§‹ä¼˜åŒ–ï¼ˆOptuna + K-Fold CVï¼‰"):
        import optuna
        from optuna.samplers import TPESampler
        
        X_train = st.session_state.X_train
        y_train = st.session_state.y_train
        
        with st.spinner(f"æ­£åœ¨è¿›è¡Œ{n_trials}æ¬¡è¯•éªŒçš„è´å¶æ–¯ä¼˜åŒ–..."):
            
            # Optunaç›®æ ‡å‡½æ•°
            def objective(trial):
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                    'max_depth': trial.suggest_int('max_depth', 3, 15),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                    'gamma': trial.suggest_float('gamma', 0, 5),
                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                    'random_state': 42
                }
                
                model = xgb.XGBRegressor(**params)
                
                # K-Foldäº¤å‰éªŒè¯
                cv_scores = cross_val_score(
                    model, X_train, y_train,
                    cv=cv_folds,
                    scoring='r2',
                    n_jobs=-1
                )
                
                return cv_scores.mean()
            
            # åˆ›å»ºstudy
            study = optuna.create_study(
                direction='maximize',
                sampler=TPESampler(seed=42)
            )
            
            # ä¼˜åŒ–
            study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
            
            # ä¿å­˜ç»“æœ
            st.session_state.best_params = study.best_params
            st.session_state.best_cv_score = study.best_value
            st.session_state.optuna_study = study
            
            st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼æœ€ä½³CV RÂ²: {study.best_value:.4f}")
            
            # æ˜¾ç¤ºæœ€ä½³å‚æ•°
            with st.expander("ğŸ“‹ æœ€ä½³å‚æ•°"):
                st.json(study.best_params)
            
            # ç»˜åˆ¶ä¼˜åŒ–å†å²
            with st.expander("ğŸ“ˆ ä¼˜åŒ–å†å²"):
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
                
                # è¯•éªŒå†å²
                trials_df = study.trials_dataframe()
                ax1.plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.3)
                ax1.plot(trials_df['number'], trials_df['value'].cummax(), 'r-', linewidth=2)
                ax1.set_xlabel('Trial Number')
                ax1.set_ylabel('CV RÂ² Score')
                ax1.set_title('Optimization History')
                ax1.legend(['Trial Score', 'Best Score'])
                ax1.grid(True, alpha=0.3)
                
                # å‚æ•°é‡è¦æ€§
                try:
                    importances = optuna.importance.get_param_importances(study)
                    importance_df = pd.DataFrame({
                        'Parameter': list(importances.keys()),
                        'Importance': list(importances.values())
                    }).sort_values('Importance', ascending=True)
                    
                    ax2.barh(importance_df['Parameter'], importance_df['Importance'])
                    ax2.set_xlabel('Importance')
                    ax2.set_title('Hyperparameter Importance')
                except:
                    ax2.text(0.5, 0.5, 'Not enough trials', ha='center', va='center')
                
                plt.tight_layout()
                st.pyplot(fig)

# ===================
# Step 4: æ¨¡å‹è®­ç»ƒ
# ===================
if 'best_params' in st.session_state:
    st.header("ğŸ“ Step 4: Train Final Model")
    
    st.info(f"ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹ï¼ˆCV RÂ²: {st.session_state.best_cv_score:.4f}ï¼‰")
    
    if st.button("è®­ç»ƒæœ€ç»ˆæ¨¡å‹"):
        X_train = st.session_state.X_train
        y_train = st.session_state.y_train
        X_test = st.session_state.X_test
        y_test = st.session_state.y_test
        
        with st.spinner("è®­ç»ƒä¸­..."):
            # ä½¿ç”¨æœ€ä½³å‚æ•°åˆ›å»ºæ¨¡å‹ï¼ˆæ·»åŠ early_stopping_roundsåˆ°æ„é€ å‡½æ•°ï¼‰
            params_with_early_stopping = st.session_state.best_params.copy()
            params_with_early_stopping['early_stopping_rounds'] = 50
            
            model = xgb.XGBRegressor(**params_with_early_stopping)
            
            # è®­ç»ƒï¼ˆæ–°ç‰ˆXGBoostï¼‰
            model.fit(
                X_train, y_train,
                eval_set=[(X_test, y_test)],
                verbose=False
            )
            
            # ä¿å­˜æ¨¡å‹
            st.session_state.trained_model = model
            
            # é¢„æµ‹
            y_pred_train = model.predict(X_train)
            y_pred_test = model.predict(X_test)
            
            st.session_state.y_pred_train = y_pred_train
            st.session_state.y_pred_test = y_pred_test
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = {
                'Dataset': ['Train', 'Test'],
                'RÂ²': [
                    r2_score(y_train, y_pred_train),
                    r2_score(y_test, y_pred_test)
                ],
                'RMSE': [
                    np.sqrt(mean_squared_error(y_train, y_pred_train)),
                    np.sqrt(mean_squared_error(y_test, y_pred_test))
                ],
                'MAE': [
                    mean_absolute_error(y_train, y_pred_train),
                    mean_absolute_error(y_test, y_pred_test)
                ]
            }
            
            metrics_df = pd.DataFrame(metrics)
            st.session_state.metrics_df = metrics_df
            
            st.success("âœ… è®­ç»ƒå®Œæˆï¼")
            
            # æ˜¾ç¤ºæŒ‡æ ‡
            st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½")
            st.dataframe(metrics_df.style.format({
                'RÂ²': '{:.4f}',
                'RMSE': '{:.2f}',
                'MAE': '{:.2f}'
            }))
            
            # ========== ä½¿ç”¨Plotlyæ›¿æ¢Matplotlib (æ€§èƒ½ä¼˜åŒ–) ==========
            import plotly.graph_objects as go
            from plotly.subplots import make_subplots
            
            # åˆ›å»ºå­å›¾
            fig = make_subplots(
                rows=1, cols=2,
                subplot_titles=('Prediction vs Actual', 'Residual Plot'),
                horizontal_spacing=0.12
            )
            
            # 1. é¢„æµ‹vså®é™…
            min_val = min(y_test.min(), y_pred_test.min())
            max_val = max(y_test.max(), y_pred_test.max())
            
            fig.add_trace(
                go.Scatter(
                    x=y_test,
                    y=y_pred_test,
                    mode='markers',
                    marker=dict(size=8, color='steelblue', opacity=0.6,
                               line=dict(width=0.5, color='white')),
                    name='Test Data',
                    hovertemplate='Actual: %{x:.2f}<br>Predicted: %{y:.2f}<extra></extra>'
                ),
                row=1, col=1
            )
            
            # æ·»åŠ ç†æƒ³çº¿
            fig.add_trace(
                go.Scatter(
                    x=[min_val, max_val],
                    y=[min_val, max_val],
                    mode='lines',
                    line=dict(color='red', dash='dash', width=2),
                    name='Perfect Prediction',
                    showlegend=True
                ),
                row=1, col=1
            )
            
            # 2. æ®‹å·®å›¾
            residuals = y_test - y_pred_test
            
            fig.add_trace(
                go.Scatter(
                    x=y_pred_test,
                    y=residuals,
                    mode='markers',
                    marker=dict(size=8, color='steelblue', opacity=0.6,
                               line=dict(width=0.5, color='white')),
                    name='Residuals',
                    showlegend=False,
                    hovertemplate='Predicted: %{x:.2f}<br>Residual: %{y:.2f}<extra></extra>'
                ),
                row=1, col=2
            )
            
            # æ·»åŠ é›¶çº¿
            fig.add_trace(
                go.Scatter(
                    x=[y_pred_test.min(), y_pred_test.max()],
                    y=[0, 0],
                    mode='lines',
                    line=dict(color='red', dash='dash', width=2),
                    name='Zero Line',
                    showlegend=False
                ),
                row=1, col=2
            )
            
            # æ›´æ–°å¸ƒå±€
            fig.update_xaxes(title_text="Actual", row=1, col=1)
            fig.update_yaxes(title_text="Predicted", row=1, col=1)
            fig.update_xaxes(title_text="Predicted", row=1, col=2)
            fig.update_yaxes(title_text="Residuals", row=1, col=2)
            
            fig.update_layout(
                height=500,
                showlegend=True,
                template='plotly_white',
                hovermode='closest',
                title_text=f'Test Set Performance (RÂ²={metrics_df.loc[1, "RÂ²"]:.4f})'
            )
            
            st.plotly_chart(fig, use_container_width=True)

# ===================
# Step 5: SHAPç‰©ç†å¯è§£é‡Šæ€§åˆ†æï¼ˆå…³é”®æ­¥éª¤ï¼‰
# ===================
if 'trained_model' in st.session_state:
    st.header("ğŸ” Step 5: SHAP Physical Interpretability Analysis")
    
    st.markdown("""
    > ğŸ¯ **å…³é”®æ­¥éª¤**ï¼šåœ¨åšé€†å‘è®¾è®¡ä¹‹å‰ï¼Œå¿…é¡»ææ¸…æ¥šç‰©ç†è§„å¾‹ï¼
    > 
    > ä¾‹å¦‚ï¼š**æé«˜åŒç³»æ¸©åº¦ (Ratio_MeltingT) åˆ°åº•æ˜¯è®©ç¡¬åº¦å˜é«˜è¿˜æ˜¯å˜ä½ï¼Ÿ**
    > 
    > ä½¿ç”¨SHAPå€¼å¯ä»¥å‡†ç¡®å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œä¸ºææ–™è®¾è®¡æä¾›æ˜ç¡®æŒ‡å¯¼ã€‚
    """)
    
    if st.button("ğŸ§ª æ‰§è¡ŒSHAPåˆ†æ"):
        import shap
        
        model = st.session_state.trained_model
        X_train = st.session_state.X_train
        X_test = st.session_state.X_test
        
        with st.spinner("è®¡ç®—SHAPå€¼ï¼ˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼‰..."):
            # åˆ›å»ºexplainer
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(X_test)
            
            st.session_state.shap_explainer = explainer
            st.session_state.shap_values = shap_values
            
            st.success("âœ… SHAPåˆ†æå®Œæˆï¼")
        
        # 5.1 Summary Bar Plot
        st.subheader("ğŸ“Š Feature Importance (Mean |SHAP|)")
        fig, ax = plt.subplots(figsize=(10, max(8, len(X_test.columns) * 0.3)))
        shap.summary_plot(shap_values, X_test, plot_type="bar", show=False)
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()
        
        # 5.2 Beeswarm Plotï¼ˆæ ¸å¿ƒå¯è§†åŒ–ï¼‰
        st.subheader("ğŸ Beeswarm Plot - ç‰©ç†è§„å¾‹åˆ†æ")
        st.markdown("""
        **å¦‚ä½•è§£è¯»**ï¼š
        - **æ¨ªè½´**ï¼šSHAPå€¼ï¼ˆæ­£å€¼å¢åŠ é¢„æµ‹ï¼Œè´Ÿå€¼å‡å°‘é¢„æµ‹ï¼‰
        - **é¢œè‰²**ï¼šç‰¹å¾å€¼å¤§å°ï¼ˆçº¢è‰²=é«˜å€¼ï¼Œè“è‰²=ä½å€¼ï¼‰
        - **ç¤ºä¾‹**ï¼šå¦‚æœ`Ratio_MeltingT`çš„çº¢ç‚¹ï¼ˆé«˜å€¼ï¼‰é›†ä¸­åœ¨å³ä¾§ï¼ˆæ­£SHAPï¼‰ï¼Œè¯´æ˜æé«˜è¯¥æ¯”å€¼ä¼šå¢åŠ ç¡¬åº¦
        """)
        
        fig, ax = plt.subplots(figsize=(12, max(8, len(X_test.columns) * 0.25)))
        shap.summary_plot(shap_values, X_test, show=False, max_display=20)
        plt.tight_layout()
        st.pyplot(fig)
        plt.close()
        
        # æå–ç‰©ç†è§„å¾‹
        st.subheader("ğŸ“ è‡ªåŠ¨æå–çš„ç‰©ç†è§„å¾‹")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡SHAPå’Œæ–¹å‘æ€§
        shap_importance = np.abs(shap_values).mean(axis=0)
        top_features_idx = np.argsort(shap_importance)[-10:][::-1]
        
        insights = []
        for idx in top_features_idx:
            feat_name = X_test.columns[idx]
            feat_shap = shap_values[:, idx]
            feat_values = X_test.iloc[:, idx].values
            
            # è®¡ç®—ç›¸å…³æ€§ï¼ˆåˆ¤æ–­æ­£è´Ÿå½±å“ï¼‰
            correlation = np.corrcoef(feat_values, feat_shap)[0, 1]
            
            direction = "æ­£ç›¸å…³" if correlation > 0 else "è´Ÿç›¸å…³"
            effect = "å¢åŠ " if correlation > 0 else "é™ä½"
            
            importance = shap_importance[idx]
            
            insights.append({
                'ç‰¹å¾': feat_name,
                'é‡è¦æ€§': f"{importance:.4f}",
                'å½±å“æ–¹å‘': direction,
                f'å¯¹{st.session_state.selected_target}çš„å½±å“': 
                    f"ç‰¹å¾å€¼â†‘ â†’ é¢„æµ‹{effect}"
            })
        
        insights_df = pd.DataFrame(insights)
        st.dataframe(insights_df, use_container_width=True)
        
        # 5.3 Dependence Plotï¼ˆé€‰æ‹©æ€§å±•ç¤ºï¼‰
        st.subheader("ğŸ“ˆ Feature Dependence Analysis")
        
        top_5_features = X_test.columns[top_features_idx[:5]]
        selected_feature = st.selectbox(
            "é€‰æ‹©ç‰¹å¾æŸ¥çœ‹è¯¦ç»†ä¾èµ–å…³ç³»",
            top_5_features
        )
        
        if selected_feature:
            fig, ax = plt.subplots(figsize=(10, 6))
            shap.dependence_plot(
                selected_feature,
                shap_values,
                X_test,
                show=False
            )
            plt.tight_layout()
            st.pyplot(fig)
            plt.close()

# ===================
# Step 6: æ¨¡å‹ä¿å­˜
# ===================
if 'trained_model' in st.session_state:
    st.header("ğŸ’¾ Step 6: Save Model")
    
    import joblib
    import os
    
    # åˆ›å»ºmodelsç›®å½•
    os.makedirs('models', exist_ok=True)
    
    model_name = st.text_input(
        "æ¨¡å‹åç§°",
        value=f"XGBoost_{st.session_state.selected_target.replace(', ', '_').replace('Â·', '').replace('/', '_')}"
    )
    
    if st.button("ä¿å­˜æ¨¡å‹"):
        model_path = f"models/{model_name}.pkl"
        
        # æ‰“åŒ…æ‰€æœ‰ä¿¡æ¯
        model_package = {
            'model': st.session_state.trained_model,
            'scaler': st.session_state.scaler,
            'feature_names': list(st.session_state.X_train.columns),
            'target_name': st.session_state.selected_target,
            'best_params': st.session_state.best_params,
            'cv_score': st.session_state.best_cv_score,
            'metrics': st.session_state.metrics_df.to_dict(),
            'shap_explainer': st.session_state.get('shap_explainer')
        }
        
        joblib.dump(model_package, model_path)
        
        st.success(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        # ç”ŸæˆåŠ è½½ä»£ç ç¤ºä¾‹
        with st.expander("ğŸ’» åŠ è½½æ¨¡å‹ä»£ç ç¤ºä¾‹"):
            code = f'''
import joblib
import pandas as pd

# åŠ è½½æ¨¡å‹
model_package = joblib.load('{model_path}')

# æå–ç»„ä»¶
model = model_package['model']
scaler = model_package['scaler']
feature_names = model_package['feature_names']

# é¢„æµ‹æ–°æ•°æ®
def predict(new_data_dict):
    """
    new_data_dict: ç‰¹å¾å­—å…¸ï¼Œä¾‹å¦‚
    {{
        'Composite_MagpieData mean Number': 27.5,
        'Diff_Electronegativity': 0.8,
        ...
    }}
    """
    X_new = pd.DataFrame([new_data_dict])
    X_new = X_new[feature_names]
    
    if scaler:
        X_new = scaler.transform(X_new)
    
    prediction = model.predict(X_new)
    return prediction[0]

# ä½¿ç”¨ç¤ºä¾‹
result = predict({{...}})
print(f"é¢„æµ‹çš„{model_package['target_name']}: {{result:.2f}}")
            '''
            st.code(code, language='python')

# ä¾§è¾¹æ æ€»ç»“
with st.sidebar:
    st.header("ğŸ“Š Training Summary")
    
    if 'best_cv_score' in st.session_state:
        st.metric("æœ€ä½³CV RÂ²", f"{st.session_state.best_cv_score:.4f}")
    
    if 'metrics_df' in st.session_state:
        test_r2 = st.session_state.metrics_df.loc[1, 'RÂ²']
        st.metric("æµ‹è¯•é›† RÂ²", f"{test_r2:.4f}")
    
    if 'X_train' in st.session_state:
        st.metric("è®­ç»ƒæ ·æœ¬æ•°", len(st.session_state.X_train))
        st.metric("ç‰¹å¾æ•°", len(st.session_state.feature_names))
    
    if 'selected_target' in st.session_state:
        st.info(f"ç›®æ ‡: {st.session_state.selected_target}")
