{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEA Model Training & Analysis\n",
    "\n",
    "**æ ¸å¿ƒå†…å®¹**:\n",
    "1. æ•°æ®åŠ è½½ä¸æ¢ç´¢\n",
    "2. **K-Foldäº¤å‰éªŒè¯**ï¼ˆè¯„ä¼°çœŸå®æ³›åŒ–èƒ½åŠ›ï¼‰\n",
    "3. è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaè´å¶æ–¯ä¼˜åŒ–ï¼‰\n",
    "4. æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "5. **SHAPç‰©ç†å¯è§£é‡Šæ€§åˆ†æ**ï¼ˆæå–ææ–™è®¾è®¡è§„å¾‹ï¼‰\n",
    "6. æ¨¡å‹ä¿å­˜ä¸éƒ¨ç½²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦åº“\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ ç¯å¢ƒè®¾ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. æ•°æ®åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½GBFSç²¾ç®€åçš„ç‰¹å¾é›†\n",
    "df = pd.read_csv('d:/ML/HEAC 0.2/datasets/hea_selected_features.csv')\n",
    "\n",
    "print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "print(f\"\\nå¯ç”¨åˆ—:\\n{df.columns.tolist()}\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ è¡Œ\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®ç»Ÿè®¡\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. æ•°æ®å‡†å¤‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰ç›®æ ‡å˜é‡ï¼ˆæ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰\n",
    "TARGET = 'HV, kgf/mm2'  # å¯é€‰: 'TRS, MPa', 'KIC, MPaÂ·m1/2'\n",
    "\n",
    "# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡\n",
    "target_candidates = ['HV, kgf/mm2', 'TRS, MPa', 'KIC, MPaÂ·m1/2']\n",
    "feature_cols = [c for c in df.columns if c not in target_candidates]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# å¤„ç†ç¼ºå¤±å€¼\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "valid_idx = y.notna()\n",
    "\n",
    "X = X[valid_idx]\n",
    "y = y[valid_idx]\n",
    "\n",
    "print(f\"æœ‰æ•ˆæ ·æœ¬æ•°: {len(X)}\")\n",
    "print(f\"ç‰¹å¾æ•°: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Teståˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"è®­ç»ƒé›†: {X_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# è½¬å›DataFrameä¿ç•™åˆ—å\n",
    "X_train = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"âœ“ æ•°æ®æ ‡å‡†åŒ–å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. K-Foldäº¤å‰éªŒè¯åŸºçº¿æ¨¡å‹\n",
    "\n",
    "> âš ï¸ **é‡è¦**: å› ä¸ºæ•°æ®é‡æœ‰é™ï¼Œå¿…é¡»ä½¿ç”¨K-Foldäº¤å‰éªŒè¯æ¥è¯„ä¼°æ¨¡å‹çš„çœŸå®æ³›åŒ–èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨é»˜è®¤å‚æ•°çš„XGBoostä½œä¸ºåŸºçº¿\n",
    "baseline_model = xgb.XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5-Foldäº¤å‰éªŒè¯\n",
    "cv_scores = cross_val_score(\n",
    "    baseline_model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"5-Fold CV RÂ² Scores: {cv_scores}\")\n",
    "print(f\"Mean CV RÂ²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optunaè¶…å‚æ•°ä¼˜åŒ– + K-Fold CV"
 ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Optunaç›®æ ‡å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # K-Foldäº¤å‰éªŒè¯\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# åˆ›å»ºstudyå¹¶ä¼˜åŒ–\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42)\n",
    ")\n",
    "\n",
    "print(\"å¼€å§‹Optunaä¼˜åŒ–ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...\")\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\næœ€ä½³CV RÂ²: {study.best_value:.4f}\")\n",
    "print(f\"\\næœ€ä½³å‚æ•°:\\n{study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–ä¼˜åŒ–å†å²\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# ä¼˜åŒ–å†å²\n",
    "trials_df = study.trials_dataframe()\n",
    "ax = axes[0]\n",
    "ax.plot(trials_df['number'], trials_df['value'], 'b-', alpha=0.3, label='Trial Score')\n",
    "ax.plot(trials_df['number'], trials_df['value'].cummax(), 'r-', linewidth=2, label='Best Score')\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('CV RÂ²')\n",
    "ax.set_title('Optimization History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# å‚æ•°é‡è¦æ€§\n",
    "importances = optuna.importance.get_param_importances(study)\n",
    "importance_df = pd.DataFrame({\n",
    "    'Parameter': list(importances.keys()),\n",
    "    'Importance': list(importances.values())\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.barh(importance_df['Parameter'], importance_df['Importance'])\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Hyperparameter Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. è®­ç»ƒæœ€ç»ˆæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹\n",
    "best_model = xgb.XGBRegressor(**study.best_params)\n",
    "\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[X_test, y_test)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# é¢„æµ‹\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# è®¡ç®—æŒ‡æ ‡\n",
    "metrics = pd.DataFrame({\n",
    "    'Dataset': ['Train', 'Test'],\n",
    "    'RÂ²': [\n",
    "        r2_score(y_train, y_pred_train),\n",
    "        r2_score(y_test, y_pred_test)\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    ],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_train, y_pred_train),\n",
    "        mean_absolute_error(y_test, y_pred_test)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"æ¨¡å‹æ€§èƒ½:\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–é¢„æµ‹ç»“æœ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# é¢„æµ‹vså®é™…\n",
    "ax = axes[0]\n",
    "ax.scatter(y_test, y_pred_test, alpha=0.6, s=50)\n",
    "min_val = min(y_test.min(), y_pred_test.min())\n",
    "max_val = max(y_test.max(), y_pred_test.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')\n",
    "ax.set_title(f'Prediction vs Actual (Test RÂ²={metrics.loc[1, \"RÂ²\"]:.4f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# æ®‹å·®å›¾\n",
    "ax = axes[1]\n",
    "residuals = y_test - y_pred_test\n",
    "ax.scatter(y_pred_test, residuals, alpha=0.6, s=50)\n",
    "ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Residuals')\n",
    "ax.set_title('Residual Plot')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAPç‰©ç†å¯è§£é‡Šæ€§åˆ†æ\n",
    "\n",
    "> ğŸ¯ **å…³é”®æ­¥éª¤**: æå–ç‰©ç†è§„å¾‹ï¼Œä¸ºææ–™è®¾è®¡æä¾›æŒ‡å¯¼\n",
    ">\n",
    "> ä¾‹å¦‚: \"æé«˜åŒç³»æ¸©åº¦ (Ratio_MeltingT) åˆ°åº•æ˜¯è®©ç¡¬åº¦å˜é«˜è¿˜æ˜¯å˜ä½?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºSHAP explainer\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(\"âœ“ SHAPå€¼è®¡ç®—å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Bar Plotï¼ˆç‰¹å¾é‡è¦æ€§ï¼‰\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm Plotï¼ˆç‰©ç†è§„å¾‹æ ¸å¿ƒå¯è§†åŒ–ï¼‰\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test, max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è‡ªåŠ¨æå–ç‰©ç†è§„å¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå–Topç‰¹å¾çš„ç‰©ç†å½±å“\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "top_features_idx = np.argsort(shap_importance)[-10:][::-1]\n",
    "\n",
    "insights = []\n",
    "for idx in top_features_idx:\n",
    "    feat_name = X_test.columns[idx]\n",
    "    feat_shap = shap_values[:, idx]\n",
    "    feat_values = X_test.iloc[:, idx].values\n",
    "    \n",
    "    # è®¡ç®—ç›¸å…³æ€§ï¼ˆåˆ¤æ–­æ­£è´Ÿå½±å“ï¼‰\n",
    "    correlation = np.corrcoef(feat_values, feat_shap)[0, 1]\n",
    "    \n",
    "    direction = \"æ­£ç›¸å…³\" if correlation > 0 else \"è´Ÿç›¸å…³\"\n",
    "    effect = \"å¢åŠ \" if correlation > 0 else \"é™ä½\"\n",
    "    importance = shap_importance[idx]\n",
    "    \n",
    "    insights.append({\n",
    "        'ç‰¹å¾': feat_name,\n",
    "        'é‡è¦æ€§': f\"{importance:.4f}\",\n",
    "        'å½±å“æ–¹å‘': direction,\n",
    "        f'å¯¹{TARGET}çš„å½±å“': f\"ç‰¹å¾å€¼â†‘ â†’ é¢„æµ‹{effect}\"\n",
    "    })\n",
    "\n",
    "insights_df = pd.DataFrame(insights)\n",
    "print(\"\\nç‰©ç†è§„å¾‹æå–:\")\n",
    "insights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence Plotï¼ˆè¯¦ç»†æŸ¥çœ‹å…³é”®ç‰¹å¾ï¼‰\n",
    "# é€‰æ‹©Top 3ç‰¹å¾å±•ç¤º\n",
    "top_3_features = X_test.columns[top_features_idx[:3]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, feat in enumerate(top_3_features):\n",
    "    shap.dependence_plot(\n",
    "        feat,\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        ax=axes[i],\n",
    "        show=False\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. æ¨¡å‹ä¿å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# åˆ›å»ºmodelsç›®å½•\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# æ‰“åŒ…æ¨¡å‹\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'target_name': TARGET,\n",
    "    'best_params': study.best_params,\n",
    "    'cv_score': study.best_value,\n",
    "    'metrics': metrics.to_dict(),\n",
    "    'shap_explainer': explainer\n",
    "}\n",
    "\n",
    "# ä¿å­˜\n",
    "model_path = f\"models/XGBoost_{TARGET.replace(', ', '_').replace('Â·', '').replace('/', '_')}.pkl\"\n",
    "joblib.dump(model_package, model_path)\n",
    "\n",
    "print(f\"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. é¢„æµ‹ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹å¹¶é¢„æµ‹æ–°æ ·æœ¬\n",
    "loaded_package = joblib.load(model_path)\n",
    "\n",
    "def predict_new_sample(sample_dict):\n",
    "    \"\"\"é¢„æµ‹æ–°æ ·æœ¬\"\"\"\n",
    "    X_new = pd.DataFrame([sample_dict])\n",
    "    X_new = X_new[loaded_package['feature_names']]\n",
    "    X_new = loaded_package['scaler'].transform(X_new)\n",
    "    \n",
    "    prediction = loaded_package['model'].predict(X_new)\n",
    "    \n",
    "    return prediction[0]\n",
    "\n",
    "# ç¤ºä¾‹ï¼šä½¿ç”¨æµ‹è¯•é›†ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "sample = X_test.iloc[0].to_dict()\n",
    "prediction = predict_new_sample(sample)\n",
    "\n",
    "print(f\"é¢„æµ‹å€¼: {prediction:.2f}\")\n",
    "print(f\"å®é™…å€¼: {y_test.iloc[0]:.2f}\")\n",
    "print(f\"è¯¯å·®: {abs(prediction - y_test.iloc[0]):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "mimetype": "text/x-python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
